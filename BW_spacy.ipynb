{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7092f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1f1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_file = 'datasets/test_data.json'\n",
    "json_file = 'datasets/dataset.json'\n",
    "with open(json_file,'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52cc68ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86439, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=['text', 'hasBadWords'])\n",
    "df.rename(columns={\"hasBadWords\": \"label\"}, inplace=True)\n",
    "df.label = df.label.apply(lambda x: 'spam' if x == True else 'ham')\n",
    "# df['text'] = df['text'].apply(lambda x: str(x).split(' '))\n",
    "# df = df[:20000]\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5855f847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 11:34:07.604213: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add the TextCategorizer to the empty model\n",
    "textcat = nlp.add_pipe(\"textcat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e32a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add labels to text classifier\n",
    "textcat.add_label(\"ham\")\n",
    "textcat.add_label(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e795393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df['text'].values\n",
    "train_labels = [{'cats': {'ham': label == 'ham',\n",
    "                          'spam': label == 'spam'}} \n",
    "                for label in df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd154e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My Favorite Slut', {'cats': {'ham': True, 'spam': False}}),\n",
       " (\"girlfriends sit on each other's faces with their asses\",\n",
       "  {'cats': {'ham': True, 'spam': False}}),\n",
       " ('bound beauty kisses her girlfriend',\n",
       "  {'cats': {'ham': True, 'spam': False}})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = list(zip(train_texts, train_labels))\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62495ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "from spacy.training.example import Example\n",
    "\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Create the batch generator with batch size = 8\n",
    "batches = minibatch(train_data, size=8)\n",
    "# Iterate through minibatches\n",
    "for batch in batches:\n",
    "    # Each batch is a list of (text, label) \n",
    "    for text, labels in batch:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, labels)\n",
    "        nlp.update([example], sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7436cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 2977.051650292923}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "spacy.util.fix_random_seed(42)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "losses = {}\n",
    "for epoch in range(1): #10\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    # Create the batch generator with batch size = 8\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    \n",
    "    # Iterate through minibatches\n",
    "    for batch in batches:\n",
    "        for text, labels in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, labels)\n",
    "            nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1deec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99704236 0.00295759]\n",
      " [0.9971174  0.00288262]]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"bastinado , feet , foot , sole , soles , toe , toe , toe job , foot job footjob , foot torture , foot torment , falaka , tickle , tickling , oiled feet , hose , hosiery , pantyhose , stockings , shoes , high heels , pumps , high heeled sandals , barefoot , toe torture , bound feet , bound foot , bound toes , heels , heel , instep , arches , arch , high arches , switching , cane , caning , crop , cropping , electrical shock , hairdryer , hot foot , hotfoot , gravel walk , breasts , ass , big ass , ass shaking , fire , foot whipping , hot pavement walks , snow walks , dirty feet , dirty foot , downtown barefoot walks .\",\n",
    "         \"ass, anal, 18 & 19 yrs old, toys,butt, latina,blondes,ass spreading\" ]\n",
    "docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "# Use textcat to get the scores for each doc\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores = textcat.predict(docs)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9293aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = scores.argmax(axis=1)\n",
    "print([textcat.labels[label] for label in predicted_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7ed4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
