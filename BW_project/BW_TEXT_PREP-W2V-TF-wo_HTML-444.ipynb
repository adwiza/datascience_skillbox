{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4031bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict, Counter  # For word frequency\n",
    "\n",
    "import collections\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# import logging  # Setting up the loggings to monitor gensim\n",
    "# logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e576792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#classifiaction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    " \n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770689b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f668d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../datasets/\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data():\n",
    "    data = pd.read_json('../datasets/dataset.json')\n",
    "    mapping = {False: 0, True: 1}\n",
    "    data.replace({'hasBadWords': mapping}, inplace=True)\n",
    "    data.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "    data.rename(columns={\"text\": \"raw_text\"}, inplace=True)\n",
    "    data.drop(['violation'], axis=1, inplace=True)\n",
    "    print('Data size %d' % len(data))\n",
    "    print('Data headers %s' % data.columns.values)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecf957",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047885b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 86439\n",
      "Data headers ['raw_text' 'labels']\n"
     ]
    }
   ],
   "source": [
    "data = read_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_text(text):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    text = bs(text,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens = text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\")+ ['14000kbps', 'november', '1080p', 'email', \n",
    "                                                 '4k', 'mp4', 'error', '404', '2022'])     \n",
    "    word_tokens = [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review = \" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda row: clean_text(row.raw_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f324d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'labels']]#, 'raw_text']] # columns reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b76aef",
   "metadata": {},
   "source": [
    "# Train the Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['labels'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, window=3, min_count=1, workers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650948b4",
   "metadata": {},
   "source": [
    "# Vectorize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test = np.array([vectorize(sentence) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d06a9b",
   "metadata": {},
   "source": [
    "# Train a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4abc3a",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb849f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, pos_label=0))\n",
    "print('Recall:', recall_score(y_test, y_pred, pos_label=0))\n",
    "print('F1 score:', f1_score(y_test, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b95cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
