{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4031bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict, Counter  # For word frequency\n",
    "\n",
    "import collections\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# import logging  # Setting up the loggings to monitor gensim\n",
    "# logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e576792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#classifiaction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    " \n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770689b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f668d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../datasets/\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data():\n",
    "    data = pd.read_json('../datasets/dataset.json')\n",
    "    mapping = {False: 0, True: 1}\n",
    "    data.replace({'hasBadWords': mapping}, inplace=True)\n",
    "    data.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "    data.rename(columns={\"text\": \"raw_text\"}, inplace=True)\n",
    "    data.drop(['violation'], axis=1, inplace=True)\n",
    "    print('Data size %d' % len(data))\n",
    "    print('Data headers %s' % data.columns.values)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecf957",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047885b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 86439\n",
      "Data headers ['raw_text' 'labels']\n"
     ]
    }
   ],
   "source": [
    "data = read_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6f1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_text(text):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    text = bs(text,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens = text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\")+ ['14000kbps', 'november', '1080p', 'email', \n",
    "                                                 '4k', 'mp4', 'error', '404', '2022'])     \n",
    "    word_tokens = [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review = \" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e066efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda row: clean_text(row.raw_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f324d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Favorite Slut</td>\n",
       "      <td>0</td>\n",
       "      <td>favorite slut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriends sit on each other's faces with the...</td>\n",
       "      <td>0</td>\n",
       "      <td>girlfriend sit face ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bound beauty kisses her girlfriend</td>\n",
       "      <td>0</td>\n",
       "      <td>bound beauty kiss girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MORGAN - Anytime - Nail Painting On The Slave'...</td>\n",
       "      <td>0</td>\n",
       "      <td>morgan anytime nail painting slave face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRANSGENDER COACHING (wmv) PART 1</td>\n",
       "      <td>0</td>\n",
       "      <td>transgender coaching wmv part</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  labels   \n",
       "0                                   My Favorite Slut       0  \\\n",
       "1  girlfriends sit on each other's faces with the...       0   \n",
       "2                 bound beauty kisses her girlfriend       0   \n",
       "3  MORGAN - Anytime - Nail Painting On The Slave'...       0   \n",
       "4                  TRANSGENDER COACHING (wmv) PART 1       0   \n",
       "\n",
       "                                      text  \n",
       "0                            favorite slut  \n",
       "1                  girlfriend sit face ass  \n",
       "2             bound beauty kiss girlfriend  \n",
       "3  morgan anytime nail painting slave face  \n",
       "4            transgender coaching wmv part  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b40b1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'labels']]#, 'raw_text']] # columns reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ec9516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favorite slut</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriend sit face ass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bound beauty kiss girlfriend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>morgan anytime nail painting slave face</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transgender coaching wmv part</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text  labels\n",
       "0                            favorite slut       0\n",
       "1                  girlfriend sit face ass       0\n",
       "2             bound beauty kiss girlfriend       0\n",
       "3  morgan anytime nail painting slave face       0\n",
       "4            transgender coaching wmv part       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b76aef",
   "metadata": {},
   "source": [
    "# Train the Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a31f848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['labels'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "415c9a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((69151,), (17288,), (69151,), (17288,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3713a4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c22c5e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 14:39:49,739 : INFO : collecting all words and their counts\n",
      "2023-04-28 14:39:49,740 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-04-28 14:39:49,759 : INFO : PROGRESS: at sentence #10000, processed 217188 words, keeping 18049 word types\n",
      "2023-04-28 14:39:49,779 : INFO : PROGRESS: at sentence #20000, processed 440729 words, keeping 25928 word types\n",
      "2023-04-28 14:39:49,798 : INFO : PROGRESS: at sentence #30000, processed 654045 words, keeping 32087 word types\n",
      "2023-04-28 14:39:49,817 : INFO : PROGRESS: at sentence #40000, processed 877083 words, keeping 36272 word types\n",
      "2023-04-28 14:39:49,835 : INFO : PROGRESS: at sentence #50000, processed 1088470 words, keeping 40310 word types\n",
      "2023-04-28 14:39:49,853 : INFO : PROGRESS: at sentence #60000, processed 1307586 words, keeping 43983 word types\n",
      "2023-04-28 14:39:49,871 : INFO : collected 48001 word types from a corpus of 1501291 raw words and 69151 sentences\n",
      "2023-04-28 14:39:49,871 : INFO : Creating a fresh vocabulary\n",
      "2023-04-28 14:39:49,941 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 48001 unique words (100.00% of original 48001, drops 0)', 'datetime': '2023-04-28T14:39:49.941362', 'gensim': '4.3.1', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) \\n[Clang 11.1.0 ]', 'platform': 'macOS-13.3.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-28 14:39:49,941 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 1501291 word corpus (100.00% of original 1501291, drops 0)', 'datetime': '2023-04-28T14:39:49.941771', 'gensim': '4.3.1', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) \\n[Clang 11.1.0 ]', 'platform': 'macOS-13.3.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-28 14:39:50,034 : INFO : deleting the raw counts dictionary of 48001 items\n",
      "2023-04-28 14:39:50,035 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2023-04-28 14:39:50,035 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1403350.9963807114 word corpus (93.5%% of prior 1501291)', 'datetime': '2023-04-28T14:39:50.035612', 'gensim': '4.3.1', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) \\n[Clang 11.1.0 ]', 'platform': 'macOS-13.3.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-04-28 14:39:50,181 : INFO : estimated required memory for 48001 words and 100 dimensions: 62401300 bytes\n",
      "2023-04-28 14:39:50,182 : INFO : resetting layer weights\n",
      "2023-04-28 14:39:50,195 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-04-28T14:39:50.195369', 'gensim': '4.3.1', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) \\n[Clang 11.1.0 ]', 'platform': 'macOS-13.3.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2023-04-28 14:39:50,195 : INFO : Word2Vec lifecycle event {'msg': 'training model with -1 workers on 48001 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3 shrink_windows=True', 'datetime': '2023-04-28T14:39:50.195638', 'gensim': '4.3.1', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) \\n[Clang 11.1.0 ]', 'platform': 'macOS-13.3.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-04-28 14:39:50,204 : INFO : EPOCH 0: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2023-04-28 14:39:50,222 : WARNING : EPOCH 0: supplied example count (0) did not equal expected count (69151)\n",
      "2023-04-28 14:39:50,222 : WARNING : EPOCH 0: supplied raw word count (0) did not equal expected count (1501291)\n",
      "2023-04-28 14:39:50,235 : INFO : EPOCH 1: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2023-04-28 14:39:50,246 : WARNING : EPOCH 1: supplied example count (0) did not equal expected count (69151)\n",
      "2023-04-28 14:39:50,246 : WARNING : EPOCH 1: supplied raw word count (0) did not equal expected count (1501291)\n",
      "2023-04-28 14:39:50,253 : INFO : EPOCH 2: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2023-04-28 14:39:50,268 : WARNING : EPOCH 2: supplied example count (0) did not equal expected count (69151)\n",
      "2023-04-28 14:39:50,268 : WARNING : EPOCH 2: supplied raw word count (0) did not equal expected count (1501291)\n",
      "2023-04-28 14:39:50,275 : INFO : EPOCH 3: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2023-04-28 14:39:50,291 : WARNING : EPOCH 3: supplied example count (0) did not equal expected count (69151)\n",
      "2023-04-28 14:39:50,291 : WARNING : EPOCH 3: supplied raw word count (0) did not equal expected count (1501291)\n",
      "2023-04-28 14:39:50,298 : INFO : EPOCH 4: training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2023-04-28 14:39:50,313 : WARNING : EPOCH 4: supplied example count (0) did not equal expected count (69151)\n",
      "2023-04-28 14:39:50,314 : WARNING : EPOCH 4: supplied raw word count (0) did not equal expected count (1501291)\n",
      "2023-04-28 14:39:50,314 : INFO : Word2Vec lifecycle event {'msg': 'training on 0 raw words (0 effective words) took 0.1s, 0 effective words/s', 'datetime': '2023-04-28T14:39:50.314528', 'gensim': '4.3.1', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) \\n[Clang 11.1.0 ]', 'platform': 'macOS-13.3.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-04-28 14:39:50,314 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48001, vector_size=100, alpha=0.025>', 'datetime': '2023-04-28T14:39:50.314726', 'gensim': '4.3.1', 'python': '3.8.12 | packaged by conda-forge | (default, Sep 29 2021, 19:21:23) \\n[Clang 11.1.0 ]', 'platform': 'macOS-13.3.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, window=3, min_count=1, workers=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650948b4",
   "metadata": {},
   "source": [
    "# Vectorize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c8a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test = np.array([vectorize(sentence) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d06a9b",
   "metadata": {},
   "source": [
    "# Train a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a34f6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4abc3a",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb849f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96298010180472\n",
      "Precision: 0.96298010180472\n",
      "Recall: 1.0\n",
      "F1 score: 0.9811409712399811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, pos_label=0))\n",
    "print('Recall:', recall_score(y_test, y_pred, pos_label=0))\n",
    "print('F1 score:', f1_score(y_test, y_pred, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b95cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
