{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58442e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 13:50:04.689492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c346f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../datasets/dataset.json')\n",
    "df.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "mapping = {False: 0.0, True: 1.0}\n",
    "df.replace({'labels': mapping}, inplace=True)\n",
    "df.drop(['violation', 'labels'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153410a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86439, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd58a9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Favorite Slut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriends sit on each other's faces with the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bound beauty kisses her girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MORGAN - Anytime - Nail Painting On The Slave'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRANSGENDER COACHING (wmv) PART 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                   My Favorite Slut\n",
       "1  girlfriends sit on each other's faces with the...\n",
       "2                 bound beauty kisses her girlfriend\n",
       "3  MORGAN - Anytime - Nail Painting On The Slave'...\n",
       "4                  TRANSGENDER COACHING (wmv) PART 1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ce76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].str.contains('<div' or '<p' or 'p>' or '<b'or '<br>' or '&nbsp' or '=') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4bff997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Favorite Slut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriends sit on each other's faces with the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bound beauty kisses her girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MORGAN - Anytime - Nail Painting On The Slave'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRANSGENDER COACHING (wmv) PART 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83594</th>\n",
       "      <td>ebony,hotwife,wife,swinger,cuckold,bigass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83595</th>\n",
       "      <td>ssbhm, bhm, ffa, female fat admire, fat admire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83596</th>\n",
       "      <td>Feet in heels, sexy shoes, high heels, high he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83597</th>\n",
       "      <td>foot fetish, breeding, kinky, fetish porn, bon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83598</th>\n",
       "      <td>Fetish Content, Ellie Boulder, Ellie Boulder P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77496 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0                                       My Favorite Slut\n",
       "1      girlfriends sit on each other's faces with the...\n",
       "2                     bound beauty kisses her girlfriend\n",
       "3      MORGAN - Anytime - Nail Painting On The Slave'...\n",
       "4                      TRANSGENDER COACHING (wmv) PART 1\n",
       "...                                                  ...\n",
       "83594          ebony,hotwife,wife,swinger,cuckold,bigass\n",
       "83595  ssbhm, bhm, ffa, female fat admire, fat admire...\n",
       "83596  Feet in heels, sexy shoes, high heels, high he...\n",
       "83597  foot fetish, breeding, kinky, fetish porn, bon...\n",
       "83598  Fetish Content, Ellie Boulder, Ellie Boulder P...\n",
       "\n",
       "[77496 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8437e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5556fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1670c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 3.93 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247d48d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52201, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f7b6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c2950e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ea04ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:54:22: collecting all words and their counts\n",
      "INFO - 13:54:22: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 13:54:22: PROGRESS: at sentence #10000, processed 328890 words and 156603 word types\n",
      "INFO - 13:54:23: PROGRESS: at sentence #20000, processed 465581 words and 211672 word types\n",
      "INFO - 13:54:23: PROGRESS: at sentence #30000, processed 954008 words and 373780 word types\n",
      "INFO - 13:54:23: PROGRESS: at sentence #40000, processed 1064330 words and 401603 word types\n",
      "INFO - 13:54:24: PROGRESS: at sentence #50000, processed 1665344 words and 534786 word types\n",
      "INFO - 13:54:24: collected 542508 token types (unigram + bigrams) from a corpus of 1687878 words and 52201 sentences\n",
      "INFO - 13:54:24: merged Phrases<542508 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 13:54:24: Phrases lifecycle event {'msg': 'built Phrases<542508 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000> in 2.05s', 'datetime': '2023-03-31T13:54:24.494204', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=1, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "032f3e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:54:24: exporting phrases from Phrases<542508 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 13:54:25: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<62104 phrases, min_count=1, threshold=10.0> from Phrases<542508 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000> in 1.06s', 'datetime': '2023-03-31T13:54:25.572638', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc26f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ccd12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84423"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb27248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p',\n",
       " 'foot',\n",
       " 'fetish',\n",
       " 'ass',\n",
       " 'girl',\n",
       " 'nbsp',\n",
       " 'strong',\n",
       " 'clip',\n",
       " 'worship',\n",
       " 'video',\n",
       " 'big',\n",
       " 'br',\n",
       " 'cum',\n",
       " 'span',\n",
       " 'cock',\n",
       " 'bondage',\n",
       " 'sexy',\n",
       " 'crush',\n",
       " 'domination',\n",
       " 'sale_com',\n",
       " 'slave',\n",
       " 'pussy',\n",
       " 'href_http',\n",
       " 'femdom',\n",
       " 'www_clip',\n",
       " 'face',\n",
       " 'humiliation',\n",
       " 'fuck',\n",
       " 'toe',\n",
       " 'leg',\n",
       " 'sex',\n",
       " 'fart',\n",
       " 'want',\n",
       " 'woman',\n",
       " 'sol',\n",
       " 'shoe',\n",
       " 'black',\n",
       " 'img_src',\n",
       " 'clip_sale',\n",
       " 'high_heel',\n",
       " 'lick',\n",
       " 'boot',\n",
       " 'time',\n",
       " 'suck',\n",
       " 'amateur',\n",
       " 'pov',\n",
       " 'play',\n",
       " 'love',\n",
       " 'href_https',\n",
       " 'like',\n",
       " 'span_style',\n",
       " 'com',\n",
       " 'center',\n",
       " 'hot',\n",
       " 'milf',\n",
       " 'female_domination',\n",
       " 'mistress',\n",
       " 'pantyhose',\n",
       " 'tease',\n",
       " 'get',\n",
       " 'align_center',\n",
       " 'mouth',\n",
       " 'hard',\n",
       " 'balloon',\n",
       " 'heel',\n",
       " 'tit',\n",
       " 'studio',\n",
       " 'hand',\n",
       " 'know',\n",
       " 'br_br',\n",
       " 'target_blank',\n",
       " 'go',\n",
       " 'tickle',\n",
       " 'anal',\n",
       " 'gag',\n",
       " 'sock',\n",
       " 'bdsm',\n",
       " 'fucking',\n",
       " 'lesbian',\n",
       " 'goddess',\n",
       " 'trample',\n",
       " 'store',\n",
       " 'b',\n",
       " 'good',\n",
       " 'http_www',\n",
       " 'smell',\n",
       " 'spank',\n",
       " 'watch',\n",
       " 'dildo',\n",
       " 'man',\n",
       " 'body',\n",
       " 'bbw',\n",
       " 'masturbation',\n",
       " 'long',\n",
       " 'mp',\n",
       " 'female',\n",
       " 'start',\n",
       " 'new',\n",
       " 'tie',\n",
       " 'little']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02421fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x140254970>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bffa2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import *\n",
    "# word_freq = [i for i in counter.most_common(50)]\n",
    "# wd = WordCloud(background_color='white')\n",
    "# wd.generate_from_frequencies(dict(word_freq))\n",
    "# plt.figure()\n",
    "# plt.imshow(wd, interpolation = 'bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c175a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ed33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "577faa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:54:27: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=100, alpha=0.03>', 'datetime': '2023-03-31T13:54:27.231352', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "827d52c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:54:27: collecting all words and their counts\n",
      "INFO - 13:54:27: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 13:54:27: PROGRESS: at sentence #10000, processed 259578 words, keeping 27856 word types\n",
      "INFO - 13:54:27: PROGRESS: at sentence #20000, processed 368880 words, keeping 36517 word types\n",
      "INFO - 13:54:28: PROGRESS: at sentence #30000, processed 757011 words, keeping 54853 word types\n",
      "INFO - 13:54:28: PROGRESS: at sentence #40000, processed 845252 words, keeping 58508 word types\n",
      "INFO - 13:54:28: PROGRESS: at sentence #50000, processed 1276701 words, keeping 83664 word types\n",
      "INFO - 13:54:28: collected 84423 word types from a corpus of 1296037 raw words and 52201 sentences\n",
      "INFO - 13:54:28: Creating a fresh vocabulary\n",
      "INFO - 13:54:29: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 84423 unique words (100.00% of original 84423, drops 0)', 'datetime': '2023-03-31T13:54:29.149409', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 13:54:29: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 1296037 word corpus (100.00% of original 1296037, drops 0)', 'datetime': '2023-03-31T13:54:29.150123', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 13:54:29: deleting the raw counts dictionary of 84423 items\n",
      "INFO - 13:54:29: sample=6e-05 downsamples 822 most-common words\n",
      "INFO - 13:54:29: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 757605.624898385 word corpus (58.5%% of prior 1296037)', 'datetime': '2023-03-31T13:54:29.576189', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 13:54:30: estimated required memory for 84423 words and 100 dimensions: 109749900 bytes\n",
      "INFO - 13:54:30: resetting layer weights\n",
      "INFO - 13:54:30: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-03-31T13:54:30.341007', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.05 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "935c06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:54:30: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 84423 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2 shrink_windows=True', 'datetime': '2023-03-31T13:54:30.350702', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO - 13:54:31: EPOCH 0 - PROGRESS: at 39.97% examples, 225196 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:32: EPOCH 0 - PROGRESS: at 82.52% examples, 244225 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:33: EPOCH 0: training on 1296037 raw words (757404 effective words) took 2.9s, 265205 effective words/s\n",
      "INFO - 13:54:34: EPOCH 1 - PROGRESS: at 37.29% examples, 201541 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:35: EPOCH 1 - PROGRESS: at 54.55% examples, 212176 words/s, in_qsize 4, out_qsize 2\n",
      "INFO - 13:54:36: EPOCH 1 - PROGRESS: at 91.47% examples, 236452 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:36: EPOCH 1: training on 1296037 raw words (757634 effective words) took 3.1s, 241146 effective words/s\n",
      "INFO - 13:54:37: EPOCH 2 - PROGRESS: at 39.30% examples, 227882 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:38: EPOCH 2 - PROGRESS: at 76.13% examples, 240123 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:54:39: EPOCH 2: training on 1296037 raw words (757141 effective words) took 3.0s, 255912 effective words/s\n",
      "INFO - 13:54:40: EPOCH 3 - PROGRESS: at 40.70% examples, 247424 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:54:41: EPOCH 3 - PROGRESS: at 83.06% examples, 254706 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:42: EPOCH 3: training on 1296037 raw words (756760 effective words) took 2.8s, 266250 effective words/s\n",
      "INFO - 13:54:43: EPOCH 4 - PROGRESS: at 40.35% examples, 247984 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:44: EPOCH 4 - PROGRESS: at 70.12% examples, 238234 words/s, in_qsize 3, out_qsize 2\n",
      "INFO - 13:54:45: EPOCH 4 - PROGRESS: at 93.90% examples, 244940 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 13:54:45: EPOCH 4: training on 1296037 raw words (757425 effective words) took 3.1s, 248111 effective words/s\n",
      "INFO - 13:54:46: EPOCH 5 - PROGRESS: at 40.32% examples, 245131 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:54:47: EPOCH 5 - PROGRESS: at 83.66% examples, 264701 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:48: EPOCH 5: training on 1296037 raw words (757636 effective words) took 2.8s, 274025 effective words/s\n",
      "INFO - 13:54:49: EPOCH 6 - PROGRESS: at 40.35% examples, 248778 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 13:54:50: EPOCH 6 - PROGRESS: at 81.53% examples, 250091 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:50: EPOCH 6: training on 1296037 raw words (757901 effective words) took 2.9s, 263879 effective words/s\n",
      "INFO - 13:54:51: EPOCH 7 - PROGRESS: at 40.70% examples, 249542 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:52: EPOCH 7 - PROGRESS: at 83.58% examples, 259842 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 13:54:53: EPOCH 7: training on 1296037 raw words (757903 effective words) took 2.8s, 270271 effective words/s\n",
      "INFO - 13:54:54: EPOCH 8 - PROGRESS: at 39.28% examples, 220883 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:54:55: EPOCH 8 - PROGRESS: at 81.22% examples, 239399 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:56: EPOCH 8: training on 1296037 raw words (756932 effective words) took 3.0s, 254247 effective words/s\n",
      "INFO - 13:54:57: EPOCH 9 - PROGRESS: at 24.90% examples, 165058 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:54:58: EPOCH 9 - PROGRESS: at 54.55% examples, 210685 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 13:54:59: EPOCH 9 - PROGRESS: at 89.82% examples, 221349 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 13:55:00: EPOCH 9: training on 1296037 raw words (757372 effective words) took 3.3s, 226560 effective words/s\n",
      "INFO - 13:55:01: EPOCH 10 - PROGRESS: at 40.70% examples, 254744 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:02: EPOCH 10 - PROGRESS: at 81.22% examples, 248364 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 13:55:03: EPOCH 10: training on 1296037 raw words (758305 effective words) took 3.0s, 255364 effective words/s\n",
      "INFO - 13:55:04: EPOCH 11 - PROGRESS: at 40.35% examples, 249003 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 13:55:05: EPOCH 11 - PROGRESS: at 81.53% examples, 250178 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 13:55:05: EPOCH 11: training on 1296037 raw words (757649 effective words) took 2.9s, 260240 effective words/s\n",
      "INFO - 13:55:07: EPOCH 12 - PROGRESS: at 39.66% examples, 224924 words/s, in_qsize 3, out_qsize 1\n",
      "INFO - 13:55:08: EPOCH 12 - PROGRESS: at 70.12% examples, 233041 words/s, in_qsize 3, out_qsize 2\n",
      "INFO - 13:55:08: EPOCH 12: training on 1296037 raw words (757931 effective words) took 3.0s, 253886 effective words/s\n",
      "INFO - 13:55:10: EPOCH 13 - PROGRESS: at 40.35% examples, 247429 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 13:55:11: EPOCH 13 - PROGRESS: at 75.82% examples, 242006 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 13:55:11: EPOCH 13: training on 1296037 raw words (757815 effective words) took 2.9s, 258238 effective words/s\n",
      "INFO - 13:55:12: EPOCH 14 - PROGRESS: at 39.28% examples, 231704 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:55:13: EPOCH 14 - PROGRESS: at 81.86% examples, 251716 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 13:55:14: EPOCH 14: training on 1296037 raw words (758334 effective words) took 2.9s, 261344 effective words/s\n",
      "INFO - 13:55:15: EPOCH 15 - PROGRESS: at 38.49% examples, 213198 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 13:55:16: EPOCH 15 - PROGRESS: at 60.59% examples, 222029 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 13:55:17: EPOCH 15 - PROGRESS: at 91.55% examples, 231142 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:55:18: EPOCH 15: training on 1296037 raw words (757510 effective words) took 3.2s, 236919 effective words/s\n",
      "INFO - 13:55:19: EPOCH 16 - PROGRESS: at 38.92% examples, 223232 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 13:55:20: EPOCH 16 - PROGRESS: at 59.41% examples, 200653 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:21: EPOCH 16 - PROGRESS: at 90.69% examples, 215257 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:55:21: EPOCH 16: training on 1296037 raw words (758192 effective words) took 3.4s, 221429 effective words/s\n",
      "INFO - 13:55:22: EPOCH 17 - PROGRESS: at 37.71% examples, 207761 words/s, in_qsize 1, out_qsize 2\n",
      "INFO - 13:55:23: EPOCH 17 - PROGRESS: at 61.80% examples, 227500 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 13:55:24: EPOCH 17 - PROGRESS: at 91.47% examples, 236311 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:24: EPOCH 17: training on 1296037 raw words (757224 effective words) took 3.1s, 243200 effective words/s\n",
      "INFO - 13:55:25: EPOCH 18 - PROGRESS: at 37.29% examples, 200161 words/s, in_qsize 3, out_qsize 1\n",
      "INFO - 13:55:26: EPOCH 18 - PROGRESS: at 59.41% examples, 220413 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 13:55:27: EPOCH 18 - PROGRESS: at 91.47% examples, 234204 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 13:55:27: EPOCH 18: training on 1296037 raw words (757646 effective words) took 3.2s, 239252 effective words/s\n",
      "INFO - 13:55:28: EPOCH 19 - PROGRESS: at 38.92% examples, 219251 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:29: EPOCH 19 - PROGRESS: at 62.97% examples, 227277 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 13:55:30: EPOCH 19 - PROGRESS: at 91.84% examples, 235939 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 13:55:30: EPOCH 19: training on 1296037 raw words (757660 effective words) took 3.2s, 238430 effective words/s\n",
      "INFO - 13:55:32: EPOCH 20 - PROGRESS: at 39.66% examples, 229436 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 13:55:33: EPOCH 20 - PROGRESS: at 64.17% examples, 228636 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:33: EPOCH 20: training on 1296037 raw words (757528 effective words) took 3.0s, 251792 effective words/s\n",
      "INFO - 13:55:35: EPOCH 21 - PROGRESS: at 39.66% examples, 227767 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:36: EPOCH 21 - PROGRESS: at 70.12% examples, 232073 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:36: EPOCH 21: training on 1296037 raw words (757996 effective words) took 3.0s, 253270 effective words/s\n",
      "INFO - 13:55:38: EPOCH 22 - PROGRESS: at 40.32% examples, 243227 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 13:55:39: EPOCH 22 - PROGRESS: at 83.06% examples, 251994 words/s, in_qsize 3, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:55:39: EPOCH 22: training on 1296037 raw words (757715 effective words) took 2.9s, 262731 effective words/s\n",
      "INFO - 13:55:40: EPOCH 23 - PROGRESS: at 39.97% examples, 240237 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 13:55:41: EPOCH 23 - PROGRESS: at 81.22% examples, 246397 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:42: EPOCH 23: training on 1296037 raw words (757788 effective words) took 2.9s, 261475 effective words/s\n",
      "INFO - 13:55:43: EPOCH 24 - PROGRESS: at 38.09% examples, 208130 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 13:55:45: EPOCH 24 - PROGRESS: at 58.26% examples, 201975 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 13:55:46: EPOCH 24 - PROGRESS: at 91.55% examples, 221387 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:46: EPOCH 24: training on 1296037 raw words (757714 effective words) took 3.3s, 228148 effective words/s\n",
      "INFO - 13:55:47: EPOCH 25 - PROGRESS: at 38.54% examples, 215204 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:55:48: EPOCH 25 - PROGRESS: at 60.62% examples, 223548 words/s, in_qsize 5, out_qsize 1\n",
      "INFO - 13:55:49: EPOCH 25 - PROGRESS: at 96.96% examples, 245305 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 13:55:49: EPOCH 25: training on 1296037 raw words (757892 effective words) took 3.1s, 247559 effective words/s\n",
      "INFO - 13:55:50: EPOCH 26 - PROGRESS: at 39.66% examples, 235562 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:51: EPOCH 26 - PROGRESS: at 79.51% examples, 244193 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 13:55:52: EPOCH 26: training on 1296037 raw words (757340 effective words) took 2.9s, 259924 effective words/s\n",
      "INFO - 13:55:53: EPOCH 27 - PROGRESS: at 38.90% examples, 223605 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 13:55:54: EPOCH 27 - PROGRESS: at 63.00% examples, 230320 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 13:55:55: EPOCH 27 - PROGRESS: at 90.69% examples, 231055 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:55: EPOCH 27: training on 1296037 raw words (758055 effective words) took 3.2s, 237006 effective words/s\n",
      "INFO - 13:55:56: EPOCH 28 - PROGRESS: at 37.29% examples, 199044 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 13:55:57: EPOCH 28 - PROGRESS: at 60.59% examples, 221090 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 13:55:58: EPOCH 28 - PROGRESS: at 90.19% examples, 224227 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 13:55:58: EPOCH 28: training on 1296037 raw words (757822 effective words) took 3.3s, 232940 effective words/s\n",
      "INFO - 13:55:59: EPOCH 29 - PROGRESS: at 40.35% examples, 246629 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 13:56:00: EPOCH 29 - PROGRESS: at 65.27% examples, 232841 words/s, in_qsize 6, out_qsize 1\n",
      "INFO - 13:56:01: EPOCH 29 - PROGRESS: at 92.82% examples, 241709 words/s, in_qsize 4, out_qsize 0\n",
      "INFO - 13:56:01: EPOCH 29: training on 1296037 raw words (757061 effective words) took 3.1s, 245294 effective words/s\n",
      "INFO - 13:56:01: Word2Vec lifecycle event {'msg': 'training on 38881110 raw words (22729285 effective words) took 91.4s, 248712 effective words/s', 'datetime': '2023-03-31T13:56:01.742099', 'gensim': '4.3.0', 'python': '3.10.10 (main, Feb 16 2023, 02:55:02) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-13.2.1-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.52 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfeb0e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/2c6bnjk559g2btv241j7f9lh0000gn/T/ipykernel_4118/514372312.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 13:56:01: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "121d0a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whore', 0.7729179859161377),\n",
       " ('couple_maledom', 0.7516284584999084),\n",
       " ('wife_hitchhiker', 0.7387457489967346),\n",
       " ('escort', 0.7235120534896851),\n",
       " ('pimp', 0.714429497718811),\n",
       " ('cum_dumpster', 0.7086385488510132),\n",
       " ('cocksucke', 0.7047537565231323),\n",
       " ('cum_dump', 0.6967712044715881),\n",
       " ('gurl', 0.6931356191635132),\n",
       " ('hotwife', 0.691525399684906)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"slut\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeb77ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578677"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity(\"slut\", 'prostitute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4351f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whore'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['cocksucke', 'couple_maledom', 'whore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddabf33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
