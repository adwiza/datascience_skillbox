{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df91d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T08:21:45.677608Z",
     "start_time": "2023-03-06T08:21:41.796062Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "import matplotlib. pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb98b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T08:21:45.868330Z",
     "start_time": "2023-03-06T08:21:45.682278Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf1780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T08:14:25.641243Z",
     "start_time": "2023-03-06T08:14:25.074911Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(\"datasets/dataset.json\")\n",
    "data.drop(['violation'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# data = shuffle(data)\n",
    "\n",
    "df = data[:80000]\n",
    "\n",
    "test =  data[80001:]# pd.read_json(\"datasets/test_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0075088",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over, y_over = oversample.fit_resample(df, df[\"hasBadWords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b984a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hasBadWords\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y = df['hasBadWords']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8965abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "punt = punctuation.replace(\"!\",\"\").replace(\"?\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize(text):\n",
    "    tokenizer = word_tokenize(text, language='english', preserve_line=False)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = tokenizer #.tokenize(contractions.fix(text))\n",
    "    \n",
    "    pruned = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in punctuation and not token.startswith(\"http\"):\n",
    "            if token.startswith(\"@\"):\n",
    "                token = token.replace(\"@\",\"\")\n",
    "                pruned.append(lemmatizer.lemmatize(token.lower()))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                pruned.append(lemmatizer.lemmatize(token.lower()))\n",
    "            \n",
    "    return pruned\n",
    "clean_text = df[\"text\"].apply(lambda x: text_tokenize(x))\n",
    "clean_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87027337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(clean_text, df[\"hasBadWords\"], test_size=0.3, random_state=2023, stratify=df[\"hasBadWords\"])\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b42224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad966381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape column \n",
    "y_train = y_train.to_numpy().reshape(-1,1)\n",
    "y_test = y_test.to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encode = OneHotEncoder(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec932e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = one_hot_encode.fit_transform(y_train)\n",
    "y_test = one_hot_encode.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98352c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_list()\n",
    "X_test = X_test.to_list()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_input_length = np.max([len(x) for x in X_train])\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding= \"post\", maxlen = max_input_length)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding= \"post\", maxlen = max_input_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221c841",
   "metadata": {},
   "source": [
    "# Import LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Dropout, Bidirectional, GRU,SimpleRNN,Embedding\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87662918",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = len(tokenizer.word_index) + 1\n",
    "input_data = Input((max_input_length,), name= \"input_layer\")\n",
    "embedding_output = Embedding(max_vocab, 100, mask_zero = True, name = \"embedding_layer\")(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319f1d6",
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=50, decay_rate=0.9, staircase=False)\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.weight_decay(tf.float32)\n",
    "    return lr\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64,input_shape = (8952,), activation = 'relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Dense(2,activation='tanh'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11499e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size = 512, epochs = 4, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd59878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('models/lstm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b58ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4600340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and toknize to test data\n",
    "text = test[\"text\"]\n",
    "text = text.apply(lambda x : text_tokenize(x))\n",
    "text = text.to_list()\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "text = tf.keras.preprocessing.sequence.pad_sequences(text, padding = \"post\", maxlen = max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ce6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions \n",
    "pred = model.predict(text)\n",
    "hasBadWords = one_hot_encode.inverse_transform(pred)\n",
    "test[\"Prediction\"] = hasBadWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0143cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(test[\"hasBadWords\"] == test[\"Prediction\"]) / len(test)\n",
    "print(\"accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a47198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong prediction \n",
    "# (test[test[\"hasBadWords\"] != test[\"Prediction\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db313bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
