{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4031bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selectolax.parser import HTMLParser\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict, Counter  # For word frequency\n",
    "\n",
    "import collections\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# import logging  # Setting up the loggings to monitor gensim\n",
    "# logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#classifiaction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    " \n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770689b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f668d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../datasets/\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd58db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data():\n",
    "    data = pd.read_json('../datasets/dataset.json')\n",
    "    mapping = {False: 0, True: 1}\n",
    "    data.replace({'hasBadWords': mapping}, inplace=True)\n",
    "    data.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "    data.rename(columns={\"text\": \"raw_text\"}, inplace=True)\n",
    "    data.drop(['violation'], axis=1, inplace=True)\n",
    "    print('Data size %d' % len(data))\n",
    "    print('Data headers %s' % data.columns.values)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecf957",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047885b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f4c06",
   "metadata": {
    "code_folding": [
     0,
     10
    ]
   },
   "outputs": [],
   "source": [
    "# def preprocess(text, stop_words, punctuation_marks): #, morph):\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "#     preprocessed_text = []\n",
    "#     for token in tokens:\n",
    "#         if token not in punctuation_marks:\n",
    "#             lemma = token #morph.parse(token)[0].normal_form\n",
    "#             if lemma not in stop_words:\n",
    "#                 preprocessed_text.append(lemma)\n",
    "#     return ' '.join(preprocessed_text)\n",
    "\n",
    "# punctuation_marks = ['!', ',', ';', ':', '(', ')', '-', '--', '?', '@', '....', '~',\n",
    "#                      '.', '..', '...', '....................', '<', '>', '=', '»', '|', '’', '`', '+', '$',\n",
    "#                      '&', '#', '+++', '*', '``', '%', '[', ']', '{', '}', '√©', '®']\n",
    "\n",
    "# stop_words = stopwords.words('english') + ['14000kbps', 'november', '1080p', '4k', 'mp4', 'error', '404', '2022']\n",
    "# morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f1545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_text(text):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    text = bs(text,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens= text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le=WordNetLemmatizer()\n",
    "    stop_words= set(stopwords.words(\"english\")+ ['14000kbps', 'november', '1080p', 'email', \n",
    "                                                 '4k', 'mp4', 'error', '404', '2022'])     \n",
    "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review=\" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda row: clean_text(row.raw_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f324d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'labels']]#, 'raw_text']] # columns reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('../datasets/clear_text.csv', index=False)\n",
    "# data.to_csv('../datasets/bw/1.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df6a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = tf.keras.utils.get_file('clear_text.csv', origin='file:////Users/adwiz/Documents/Courses/datascience_skillbox/datasets/clear_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(csv_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ac6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3109881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_texts = df[df['labels'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_texts = df[df['labels'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ffa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ffa43",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eed25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = counter_word(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = num_unique_words = len(counter)\n",
    "oov_token = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683df9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      data.text,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=1)\n",
    "\n",
    "negative_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      data.text,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "    \n",
    "print(f'Positive skip_grams {len(positive_skip_grams)}')\n",
    "print(f'Negative skip_grams {len(negative_skip_grams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5255587",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c709722",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a12dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546128ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in counter.most_common(50)]\n",
    "wd = WordCloud(background_color='white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce46595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d6b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a2f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe66508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5b239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(data.shape[0] * .8)\n",
    "\n",
    "train = data[:train_size]\n",
    "val = data[train_size:]\n",
    "\n",
    "# Split train and test\n",
    "\n",
    "X_train = train.text.to_numpy()\n",
    "y_train = train.labels.to_numpy()\n",
    "\n",
    "X_val = val.text.to_numpy()\n",
    "y_val = val.labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57222bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de16bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb477a4",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72213def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize a text corpus by turning each text into sentence of integers\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_unique_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('../models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('../models/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9fb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word have a unique index\n",
    "word_index = tokenizer.word_index\n",
    "word_index['<pad>'] = 0  # add a padding token\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cac460",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = {index: token for token, index in word_index.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension so you can use concatenation (in the next step).\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "# Reshape the target to shape `(1,)` and context and label to `(num_ns+1,)`.\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a01eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4970760",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "\n",
    "def tokenize(text, unused_label):\n",
    "    lower_case = tf_text.case_fold_utf8(text)\n",
    "    return tokenizer.tokenize(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c79689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:3])\n",
    "print(X_train_sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60158bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea305f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(num_unique_words, 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.legacy.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_sequences, epochs=15,\n",
    "                    validation_data=X_val_sequences,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b66eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
