{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf54426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#for word embedding\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, cohen_kappa_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('datasets/dataset.json')\n",
    "df['labels'] = df.hasBadWords.apply(lambda x: \"hasBadWords.True\" if x == True else \"hasBadWords.False\")\n",
    "# data['labels'] = df.hasBadWords.apply(lambda x: 1 if x == True else 0)\n",
    "df.drop(['violation', 'hasBadWords'], axis=1, inplace=True)\n",
    "# df['text'] = df['text'].apply(lambda x: str(x).split(' '))\n",
    "df['text'] = df['text'].apply(lambda x: str(x))\n",
    "# df.rename(columns={\"text\": \"texts\"}, inplace=True)\n",
    "df['labels'] = df['labels'].astype('category')\n",
    "print('dimension: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile('[\\w|\\d]+') #('[A-Za-z]+')\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    return ' '.join(regex.findall(text))\n",
    "\n",
    "df.text = df.text.str.lower()\n",
    "df.text = df.text.apply(words_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a88ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "mystopwords = stopwords.words('english') + ['I', 'div' , 'p', 'center', 'tr', 'com',  '</tr>', 'The', 'http',\n",
    "                                           'font', 'br', 'href', 'clips4sale', 'www', 'align', 'td', \n",
    "                                            'img', 'src', 'class', 'span', '0', 'nbsp', 'b', 'imagecdn',\n",
    "                                           '_blank', 'color', 'target', 'width', 'https://', 'jpg', 'bottom_html',\n",
    "                                           'bottom_text', 'style', 'text', 'link', 'https', 'images', 'size',\n",
    "                                           'border', 'height', 'true ', 'return true', 'wmv', 'sans', 'window status',\n",
    "                                           'arial', 'helvetica']\n",
    "def remove_stopwords(text, mystopwords=mystopwords):\n",
    "    try:\n",
    "        return ' '.join([token for token in text.split() if not token in mystopwords])\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "df.text = df.text.apply(remove_stopwords)\n",
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15a454e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymystem3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymystem3'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Unworked shit\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "m = Mystem()\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return ''.join(m.lemmatize(text)).strip()\n",
    "    except:\n",
    "        return ' '\n",
    "\n",
    "##### Alternative way\n",
    "\n",
    "# import nltk\n",
    "\n",
    "# w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# def lemmatize(text):\n",
    "#     return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "df.text = df.text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8b6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.0-cp310-cp310-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/adwiz/venv310/lib/python3.10/site-packages (from gensim) (1.24.2)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Using cached FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/adwiz/venv310/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: pandas in /Users/adwiz/venv310/lib/python3.10/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume\n",
      "  Using cached pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/adwiz/venv310/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/adwiz/venv310/lib/python3.10/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Collecting fst-pso\n",
      "  Using cached fst_pso-1.8.1-py3-none-any.whl\n",
      "Collecting simpful\n",
      "  Downloading simpful-2.10.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/adwiz/venv310/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful\n",
      "  Using cached miniful-0.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /Users/adwiz/venv310/lib/python3.10/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.28.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adwiz/venv310/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/adwiz/venv310/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adwiz/venv310/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adwiz/venv310/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.4)\n",
      "Installing collected packages: smart-open, simpful, miniful, fst-pso, pyfume, FuzzyTM, gensim\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 gensim-4.3.0 miniful-0.0.6 pyfume-0.2.25 simpful-2.10.0 smart-open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mystoplemmas = ['font','br','href', 'clips4sale', 'www', 'td', 'align', 'class', 'span']\n",
    "# def remove_stoplemmas(text, mystoplemmas=mystoplemmas):\n",
    "#     try:\n",
    "#         return ' '.join([token for token in text.split() if not token in mystoplemmas])\n",
    "#     except:\n",
    "#         return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c617f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df['labels'].astype('category')\n",
    "print('dimension: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = df['labels'].cat.categories\n",
    "df['labels'] = df['labels'].cat.codes\n",
    "X = df['text']\n",
    "y = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 1234\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "lemmata = []\n",
    "for index, row in df.iterrows():\n",
    "    lemmata  += row['text'].split()\n",
    "fd = FreqDist(lemmata)\n",
    "for i in fd.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in fd.most_common(100)]\n",
    "wd = WordCloud(background_color='white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f019f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_by_topic = []\n",
    "for topic in df['labels'].unique():\n",
    "    tokens = []\n",
    "    sample = df[df['labels'] == topic]\n",
    "    for i in range(len(sample)):\n",
    "        tokens += sample.text.iloc[i].split()\n",
    "    tokens_by_topic.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f95589",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = 1\n",
    "df['labels'].unique()[event_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "from nltk import *\n",
    "N_best = 100 # число извлекаемых биграмм\n",
    "\n",
    "bigram_mesures = nltk.collocations.BigramAssocMeasures() # класс для мер ассоциации биграмм\n",
    "finder = BigramCollocationFinder.from_words(tokens_by_topic[event_id]) # класс для хранения и извлечение биграм\n",
    "finder.apply_freq_filter(3) # избавимся от биграм, которые встречаются реже 3 раз\n",
    "raw_freq_ranking = [' '.join(i) for i in finder.nbest(bigram_mesures.raw_freq, N_best)] # выбираем топ 10 биграм по частоте\n",
    "tscore_ranking = [' '.join(i) for i in finder.nbest(bigram_mesures.student_t, N_best)] # выбираем топ 10 биграм по категориям\n",
    "pmi_ranking = [' '.join(i) for i in finder.nbest(bigram_mesures.pmi, N_best)]\n",
    "llr_ranking = [' '.join(i) for i in finder.nbest(bigram_mesures.likelihood_ratio, N_best)]\n",
    "chi2_ranking = [' '.join(i) for i in finder.nbest(bigram_mesures.chi_sq, N_best)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = pd.DataFrame({'chi2': chi2_ranking, 'llr': llr_ranking, 't-score': tscore_ranking, 'pmi': pmi_ranking, 'raw': raw_freq_ranking})\n",
    "rankings = rankings[['raw', 'pmi', 't-score', 'chi2', 'llr']]\n",
    "rankings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81777fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "corr = spearmanr(rankings).correlation\n",
    "sns.heatmap(corr, annot=True, xticklabels = list(rankings), yticklabels = list(rankings));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fac91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec7550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer + tokenizer (+ stemming) class\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        # we define (but not use) a stemming method, uncomment the last line in __call__ to get stemming tooo\n",
    "        self.stemmer = nltk.stem.SnowballStemmer('english') \n",
    "    def __call__(self, doc):\n",
    "        # pattern for numbers | words of length=2 | punctuations | words of length=1\n",
    "        pattern = re.compile(r'[0-9]+|\\b[\\w]{2,2}\\b|[%.,_`!\"&?\\')({~@;:#}+-]+|\\b[\\w]{1,1}\\b')\n",
    "        # tokenize document\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        #filter out patterns from words\n",
    "        doc_tok = [x for x in doc_tok if x not in stop_words1]\n",
    "        doc_tok = [pattern.sub('', x) for x in doc_tok]\n",
    "        # get rid of anything with length=1\n",
    "        doc_tok = [x for x in doc_tok if len(x) > 1]\n",
    "        # position tagging\n",
    "        doc_tagged = nltk.pos_tag(doc_tok)\n",
    "        # selecting nouns and adjectives\n",
    "        doc_tagged = [(t[0], t[1]) for t in doc_tagged if t[1] in defTags]\n",
    "        # preparing lemmatization\n",
    "        doc = [(t[0], penn_to_wn(t[1])) for t in doc_tagged]\n",
    "        # lemmatization\n",
    "        doc = [self.wnl.lemmatize(t[0], t[1]) for t in doc]\n",
    "        # uncomment if you want stemming as well\n",
    "        #doc = [self.stemmer.stem(x) for x in doc]\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db058c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tdidf = TfidfVectorizer(ngram_range=(1,1), analyzer='word', #stop_words=stop_words1, \n",
    "                                               norm='l2', tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6ca85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15c087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c5610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ec5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_mapping = data['labels'].cat.categories\n",
    "# data['labels'] = data['labels'].cat.codes\n",
    "# X = data['texts']\n",
    "# y = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ec868",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 1234\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n",
    "# val_size = 0.1\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X_train, y_train, test_size=val_size, random_state=random_state, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474b320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
