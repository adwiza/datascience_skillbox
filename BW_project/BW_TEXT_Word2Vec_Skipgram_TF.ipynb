{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043fc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import pandas as pd \n",
    "import six\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f669f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/adwiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc69dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App\n",
      "Machine-Learning-Book-Ratings\n",
      "anna_words.txt\n",
      "bad_words.txt\n",
      "bank-full.csv\n",
      "bw.csv\n",
      "cat_pred.csv\n",
      "check_false_sentenses.csv\n",
      "check_sentenses.csv\n",
      "clear_text.csv\n",
      "coin_prices.db\n",
      "data.txt\n",
      "dataset.json\n",
      "dependent_t.csv\n",
      "df.parquet\n",
      "df4.xlsx\n",
      "df5.csv\n",
      "df6.json\n",
      "df7.csv\n",
      "df_clean.csv\n",
      "df_out.csv\n",
      "df_types.csv\n",
      "df_types_upd.csv\n",
      "form_LP001014.json\n",
      "form_LP001024.json\n",
      "iTunes_api.csv\n",
      "iTunes_api.xlsx\n",
      "independent_t_student.csv\n",
      "insurance.csv\n",
      "last_one.csv\n",
      "last_one.json\n",
      "loan_train.csv\n",
      "main.py\n",
      "mann_whitney.csv\n",
      "neil_ProducerClipSite_rand.json\n",
      "other_partner_data.csv\n",
      "partner_data.csv\n",
      "partner_data_next_part.csv\n",
      "partner_data_records.json\n",
      "partner_data_records_cp1251.json\n",
      "partner_data_semicolon.csv\n",
      "pics\n",
      "pima-indians-diabetes.data.csv\n",
      "pima-indians-diabetes.data.csv.1\n",
      "pima-indians-diabetes.data.csv.2\n",
      "practice4.csv\n",
      "practice_5.1.csv\n",
      "practice_5.2.csv\n",
      "prance-8.jpg\n",
      "recdemo.csv\n",
      "signed_wilcoxon.csv\n",
      "stroke_data.csv\n",
      "stroke_data_encoded.csv\n",
      "submission.csv\n",
      "test.csv\n",
      "test_data.json\n",
      "transfusion_main.csv\n",
      "transfusion_oot.csv\n",
      "vehicles_dataset.csv\n",
      "vehicles_dataset_prepared.csv\n",
      "vehicles_dataset_upd.csv\n",
      "vehicles_dataset_upd2.csv\n",
      "vehicles_dataset_upd3.csv\n",
      "vehicles_dataset_upd4.csv\n",
      "vehicles_dataset_upd5.csv\n",
      "vehicles_dataset_upd6.csv\n",
      "wo_html.csv\n",
      "Когортный анализ.xlsx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../datasets/\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db35f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data():\n",
    "    df_train = pd.read_json('../datasets/dataset.json')\n",
    "    mapping = {False: 0, True: 1}\n",
    "    df_train.replace({'hasBadWords': mapping}, inplace=True)\n",
    "    df_train.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "    df_train.drop(['violation'], axis=1, inplace=True)\n",
    "    print('Data size %d' % len(df_train))\n",
    "    print('Data headers %s' % df_train.columns.values)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e68c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 86439\n",
      "Data headers ['text' 'labels']\n"
     ]
    }
   ],
   "source": [
    "df_train = read_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5dd52e",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95fcd99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stop_words, punctuation_marks): #, morph):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    preprocessed_text = []\n",
    "    for token in tokens:\n",
    "        if token not in punctuation_marks:\n",
    "            lemma = token #morph.parse(token)[0].normal_form\n",
    "            if lemma not in stop_words:\n",
    "                preprocessed_text.append(lemma)\n",
    "    return ' '.join(preprocessed_text)\n",
    "\n",
    "punctuation_marks = ['!', ',', ';', ':', '(', ')', '-', '--', '?', '@', '....', '~',\n",
    "                     '.', '..', '...', '....................', '<', '>', '=', '»', '|', '’', '`', '+', '$',\n",
    "                     '&', '#', '+++', '*', '``', '%', '[', ']', '{', '}', '√©']\n",
    "\n",
    "stop_words = stopwords.words('english') + ['14000kbps', 'november', '1080p', '4k', 'mp4', 'error', '404', '2022']\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f075a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows with \"<div\"\n",
    "# identify partial string\n",
    "discard = [\"<div \", \"<p \", \"<span \", \"<p>\", \"<div>\", \"<h\", \"<input \", \"center>\", \"<a \", \n",
    "           \"<td>\", \"<\", \">\", r\"              \", \"Ø\", '√ú', 'http://']\n",
    "  \n",
    "df_train = df_train[~df_train.text.str.contains('|'.join(discard))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f5b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.apply(lambda row: bs(row.text, 'lxml').get_text().replace('\\r\\n', ' ').replace('/', ' ').replace('\"', '\\\"'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991274fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.apply(lambda row: preprocess(row.text, punctuation_marks, stop_words), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213cc94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favorite slut</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriends sit 's faces asses</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bound beauty kisses girlfriend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>morgan anytime nail painting slave 's face</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transgender coaching wmv part 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text  labels\n",
       "0                               favorite slut       0\n",
       "1              girlfriends sit 's faces asses       0\n",
       "2              bound beauty kisses girlfriend       0\n",
       "3  morgan anytime nail painting slave 's face       0\n",
       "4             transgender coaching wmv part 1       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e44c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 941214\n"
     ]
    }
   ],
   "source": [
    "def extract_words(df_train):\n",
    "    words = list()\n",
    "    for index, row in df_train.iterrows():\n",
    "        q1 = row['text']\n",
    "        if not q1 or not isinstance(q1, six.string_types):\n",
    "            continue\n",
    "        q_words = q1.split()\n",
    "        for word in q_words:\n",
    "            words.append(word)\n",
    "    return words\n",
    "vocabulary_size = 500000\n",
    "words = extract_words(df_train)\n",
    "print('Number of words: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c9de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of count\n",
      "46058\n",
      "Most common words (+UNK) [['UNK', 0], ('fetish', 21782), ('foot', 16165), ('feet', 13306), ('domination', 9319)]\n",
      "Sample data [2229, 110, 1845, 709, 39, 887, 761, 132, 550, 1353]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    print(\"size of count\")\n",
    "    print(len(count))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9de2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6ae1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bda67077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['favorite', 'slut', 'girlfriends', 'sit', \"'s\", 'faces', 'asses', 'bound']\n"
     ]
    }
   ],
   "source": [
    "print('data:', [reverse_dictionary[di] for di in data[:8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68a29593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "    batch: ['slut', 'slut', 'girlfriends', 'girlfriends', 'sit', 'sit', \"'s\", \"'s\"]\n",
      "    labels: ['girlfriends', 'favorite', 'sit', 'slut', \"'s\", 'girlfriends', 'faces', 'sit']\n",
      "\n",
      "with num_skips = 4 and skip_window = 2:\n",
      "    batch: ['girlfriends', 'girlfriends', 'girlfriends', 'girlfriends', 'sit', 'sit', 'sit', 'sit']\n",
      "    labels: ['sit', 'favorite', 'slut', \"'s\", 'girlfriends', 'slut', 'faces', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03120caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 4  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c09341b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of tensors similarity, embeddings, norm, normalized_embeddings, valid_embeddings\n",
      "Tensor(\"MatMul_2:0\", shape=(16, 500000), dtype=float32, device=/device:CPU:0)\n",
      "<tf.Variable 'Variable_6:0' shape=(500000, 128) dtype=float32>\n",
      "Tensor(\"Sqrt_2:0\", shape=(500000, 1), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"truediv_2:0\", shape=(500000, 128), dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"embedding_lookup_5/Identity:0\", shape=(16, 128), dtype=float32, device=/device:CPU:0)\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    # Input data.\n",
    "    train_dataset = tf.compat.v1.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random.uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.random.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities\n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.keras.optimizers.legacy.Adagrad()\n",
    "\n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "        normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "    print(\"Shapes of tensors similarity, embeddings, norm, normalized_embeddings, valid_embeddings\")\n",
    "    print(similarity)\n",
    "    print(embeddings)\n",
    "    print(norm)\n",
    "    print(normalized_embeddings)\n",
    "    print(valid_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "238030b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00586ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `fetch` = <keras.optimizers.legacy.adagrad.Adagrad object at 0x16fa31ac0> has invalid type \"Adagrad\" must be a string or Tensor. (Can not convert a Adagrad into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:304\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unique_fetches\u001b[38;5;241m.\u001b[39mappend(\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_graph_element\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_operation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:4012\u001b[0m, in \u001b[0;36mGraph.as_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   4011\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 4012\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_graph_element_locked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_operation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:4100\u001b[0m, in \u001b[0;36mGraph._as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4099\u001b[0m   \u001b[38;5;66;03m# We give up!\u001b[39;00m\n\u001b[0;32m-> 4100\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not convert a \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m into a \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   4101\u001b[0m                   (\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, types_str))\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a Adagrad into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_data, batch_labels \u001b[38;5;241m=\u001b[39m generate_batch(\n\u001b[1;32m      7\u001b[0m       batch_size, num_skips, skip_window)\n\u001b[1;32m      8\u001b[0m feed_dict \u001b[38;5;241m=\u001b[39m {train_dataset : batch_data, train_labels : batch_labels}\n\u001b[0;32m----> 9\u001b[0m _, l \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m average_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:1176\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       feed_map[compat\u001b[38;5;241m.\u001b[39mas_bytes(subfeed_t\u001b[38;5;241m.\u001b[39mname)] \u001b[38;5;241m=\u001b[39m (subfeed_t, subfeed_val)\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# Create a fetch handler to take care of the structure of fetches.\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m fetch_handler \u001b[38;5;241m=\u001b[39m \u001b[43m_FetchHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_handles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_handles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;66;03m# Run request and get response.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;66;03m# We need to keep the returned movers alive for the following _do_run().\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;66;03m# These movers are no longer needed when _do_run() completes, and\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# are deleted when `movers` goes out of scope when this _run() ends.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# TODO(yuanbyu, keveman): Revisit whether we should just treat feeding\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# of a handle from a different device as an error.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_with_movers(feed_dict_tensor, feed_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:485\u001b[0m, in \u001b[0;36m_FetchHandler.__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a fetch handler.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    direct feeds.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 485\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_mapper \u001b[38;5;241m=\u001b[39m \u001b[43m_FetchMapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:266\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has invalid type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    263\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(fetch)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Cannot be None\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fetch, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    265\u001b[0m   \u001b[38;5;66;03m# NOTE(touts): This is also the code path for namedtuples.\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ListFetchMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fetch, collections_abc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    268\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _DictFetchMapper(fetch)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:378\u001b[0m, in \u001b[0;36m_ListFetchMapper.__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(fetches)\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mappers \u001b[38;5;241m=\u001b[39m [_FetchMapper\u001b[38;5;241m.\u001b[39mfor_fetch(fetch) \u001b[38;5;28;01mfor\u001b[39;00m fetch \u001b[38;5;129;01min\u001b[39;00m fetches]\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unique_fetches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_indices \u001b[38;5;241m=\u001b[39m _uniquify_fetches(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mappers)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:378\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(fetches)\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mappers \u001b[38;5;241m=\u001b[39m [\u001b[43m_FetchMapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fetch \u001b[38;5;129;01min\u001b[39;00m fetches]\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unique_fetches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_indices \u001b[38;5;241m=\u001b[39m _uniquify_fetches(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mappers)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:276\u001b[0m, in \u001b[0;36m_FetchMapper.for_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fetch, tensor_type):\n\u001b[1;32m    275\u001b[0m       fetches, contraction_fn \u001b[38;5;241m=\u001b[39m fetch_fn(fetch)\n\u001b[0;32m--> 276\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ElementFetchMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontraction_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# Did not find anything.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has invalid type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    279\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(fetch)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/client/session.py:307\u001b[0m, in \u001b[0;36m_ElementFetchMapper.__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unique_fetches\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39mas_graph_element(\n\u001b[1;32m    305\u001b[0m       fetch, allow_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_operation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 307\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has invalid type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    308\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(fetch)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m must be a string or Tensor. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    309\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArgument `fetch` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfetch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be interpreted as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    312\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma Tensor. (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `fetch` = <keras.optimizers.legacy.adagrad.Adagrad object at 0x16fa31ac0> has invalid type \"Adagrad\" must be a string or Tensor. (Can not convert a Adagrad into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session(graph=graph) as session:\n",
    "    tf.compat.v1.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "              batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step %d: %f' % (step, average_loss))\n",
    "                average_loss = 0\n",
    "        # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                print(nearest)\n",
    "                print(len(reverse_dictionary))\n",
    "                for k in range(top_k):\n",
    "                    a = nearest[k]\n",
    "                    print(a)\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    print(close_word)\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "        final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afdb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
