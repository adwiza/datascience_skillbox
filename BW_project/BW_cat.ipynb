{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1266ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #, category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb6a041c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86439, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categories = [\n",
    "#     \"alt.atheism\",\n",
    "#     \"misc.forsale\",\n",
    "#     \"sci.space\",\n",
    "#     \"soc.religion.christian\",\n",
    "#     \"talk.politics.guns\",\n",
    "# ]\n",
    "\n",
    "categories = [ \n",
    "    \"hasBadWords.True\",\n",
    "    \"hasBadWords.False\",\n",
    "             ]\n",
    "\n",
    "news_group_data = pd.read_json('../datasets/dataset.json') # dataset.json\n",
    "news_group_data['target'] = news_group_data.hasBadWords.apply(lambda x: \"hasBadWords.True\" if x == True else \"hasBadWords.False\")\n",
    "# news_group_data.drop(['violation'], axis=1, inplace=True)\n",
    "news_group_data.shape\n",
    "\n",
    "# news_group_data = fetch_20newsgroups(\n",
    "#     subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    "# )\n",
    "\n",
    "# df = pd.DataFrame(\n",
    "#     dict(\n",
    "#         text=news_group_data[\"text\"],\n",
    "#         target=news_group_data[\"target\"]\n",
    "#     )\n",
    "# )\n",
    "# df[\"target\"] = df.target.map(lambda x: categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f971de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    dict(\n",
    "        text=news_group_data[\"text\"],\n",
    "        target=news_group_data[\"target\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "759e5bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My Favorite Slut</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriends sit on each other's faces with the...</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bound beauty kisses her girlfriend</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MORGAN - Anytime - Nail Painting On The Slave'...</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRANSGENDER COACHING (wmv) PART 1</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             target\n",
       "0                                   My Favorite Slut  hasBadWords.False\n",
       "1  girlfriends sit on each other's faces with the...  hasBadWords.False\n",
       "2                 bound beauty kisses her girlfriend  hasBadWords.False\n",
       "3  MORGAN - Anytime - Nail Painting On The Slave'...  hasBadWords.False\n",
       "4                  TRANSGENDER COACHING (wmv) PART 1  hasBadWords.False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7235e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_text(text):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    text = bs(text,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens = text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\")+ ['14000kbps', 'november', '1080p', 'email', \n",
    "                                                 '4k', 'mp4', 'error', '404', '2022', 'hd'])     \n",
    "    word_tokens = [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review = \" \".join(word_tokens)\n",
    "    return cleaned_review\n",
    "\n",
    "df[\"clean_text\"] = df.text.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1df59b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>favorite slut</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriend sit face ass</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bound beauty kiss girlfriend</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>morgan anytime nail painting slave face</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transgender coaching wmv part</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                clean_text             target\n",
       "0                            favorite slut  hasBadWords.False\n",
       "1                  girlfriend sit face ass  hasBadWords.False\n",
       "2             bound beauty kiss girlfriend  hasBadWords.False\n",
       "3  morgan anytime nail painting slave face  hasBadWords.False\n",
       "4            transgender coaching wmv part  hasBadWords.False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['clean_text', 'target']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05656968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)#, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a360dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(\n",
    "    ngram_range=(1, 3), \n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "X_train = vec.fit_transform(df_train.clean_text)\n",
    "X_test = vec.transform(df_test.clean_text)\n",
    "\n",
    "y_train = df_train.target\n",
    "y_test = df_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4f9d045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: MultinomialNB(alpha=1)\n",
      "Каппа-коэффициент Коэна:  0.5895560923783691\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "hasBadWords.False       0.98      0.99      0.98     16632\n",
      " hasBadWords.True       0.61      0.60      0.60       656\n",
      "\n",
      "         accuracy                           0.97     17288\n",
      "        macro avg       0.80      0.79      0.79     17288\n",
      "     weighted avg       0.97      0.97      0.97     17288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param={'alpha': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]}\n",
    "\n",
    "nb = MultinomialNB()\n",
    "clf=GridSearchCV(nb, param, scoring='f1_macro', cv=10, return_train_score=True) \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Best estimator:', clf.best_estimator_)\n",
    "      \n",
    "preds = clf.predict(X_test)\n",
    "print('Каппа-коэффициент Коэна: ', cohen_kappa_score(y_test, preds))\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "915eb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(df_test.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4255b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['predict'] = new_df.apply(lambda x: preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10cc7f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76802</th>\n",
       "      <td>programmerswife</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61571</th>\n",
       "      <td>mature bbw squirt anal masturbation milf as pu...</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>marcia wood wearing black top want show new fa...</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65815</th>\n",
       "      <td>custom clip question anything u alkatrazentert...</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56334</th>\n",
       "      <td>best tape gagged damsel</td>\n",
       "      <td>hasBadWords.False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_text            predict\n",
       "76802                                    programmerswife  hasBadWords.False\n",
       "61571  mature bbw squirt anal masturbation milf as pu...  hasBadWords.False\n",
       "7279   marcia wood wearing black top want show new fa...  hasBadWords.False\n",
       "65815  custom clip question anything u alkatrazentert...  hasBadWords.False\n",
       "56334                            best tape gagged damsel  hasBadWords.False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d782509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv('datasets/cat_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6f88045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vec.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib.dump(nb, \"nb.joblib\")\n",
    "joblib.dump(clf, \"nb.joblib\")\n",
    "joblib.dump(vec, \"vec.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5718a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_saved = joblib.load(\"nb.joblib\")\n",
    "vec_saved = joblib.load(\"vec.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c53900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.86 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['hasBadWords.False'], dtype='<U17')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "# sample_text = [\"Space, Stars, Planets and Astronomy!\"]\n",
    "sample_text = [\"\"\"start\tend\ttext\n",
    "0\t6000\tMom's stuff is so pretty. She has so many cute things.\n",
    "6000\t9000\tWhich one's the most? Which one's the most?\n",
    "9000\t12000\tWhat do you want?\n",
    "12000\t15000\tI got so pretty.\n",
    "15000\t17000\tYou like the pink one.\n",
    "17000\t19000\tYou look so cute in the pink one.\n",
    "19000\t21000\tDo you like this like weird shoulder thing though?\n",
    "21000\t23000\tHow does it look?\n",
    "23000\t24000\tI don't know.\n",
    "24000\t25000\tWait, this is the dress.\n",
    "25000\t26000\tIt's a dress.\n",
    "26000\t32000\tDo you like it?\n",
    "32000\t33000\tI don't know.\n",
    "33000\t34000\tYou look so pretty.\n",
    "34000\t37000\tYou don't see?\n",
    "37000\t38000\tYeah.\n",
    "38000\t39000\tCute.\n",
    "39000\t41000\tYou look so pretty though.\n",
    "41000\t43000\tNo, you don't.\n",
    "43000\t45000\tYou want to check on me yourself?\n",
    "45000\t46000\tYeah.\n",
    "46000\t47000\tWhat's in this?\n",
    "47000\t49000\tI want to see you in the back maybe.\n",
    "49000\t50000\tYeah?\n",
    "50000\t53000\tYou can't take it off.\n",
    "53000\t61000\tI'm thinking of this.\n",
    "61000\t63000\tI'm not so cute.\n",
    "63000\t67000\tYou are.\n",
    "67000\t71000\tWait, now maybe I want to check it out.\n",
    "71000\t72000\tOkay.\n",
    "72000\t73000\tHow do we get this?\n",
    "73000\t74000\tYeah.\n",
    "74000\t76000\tHere's a cup.\n",
    "76000\t77000\tLet's go on.\n",
    "77000\t78000\tOh, yeah.\n",
    "78000\t79000\tOkay.\n",
    "79000\t80000\tOkay.\n",
    "80000\t83000\tOh, here's some free.\n",
    "83000\t84000\tDolly.\n",
    "84000\t85000\tYou can't release a box.\n",
    "85000\t86000\tYou don't stop.\n",
    "86000\t90000\tYou can just tell me you don't want to stop it everywhere.\n",
    "90000\t91000\tOkay.\n",
    "91000\t92000\tOkay.\n",
    "92000\t93000\tOkay.\n",
    "93000\t94000\tOkay.\n",
    "94000\t95000\tOkay.\n",
    "95000\t96000\tOkay.\n",
    "96000\t97000\tOkay.\n",
    "97000\t98000\tOkay.\n",
    "98000\t99000\tOkay.\n",
    "99000\t100000\tOkay.\n",
    "100000\t101000\tOkay.\n",
    "101000\t102000\tOkay.\n",
    "102000\t103000\tOkay.\n",
    "103000\t104000\tOkay.\n",
    "104000\t105000\tOkay.\n",
    "105000\t106000\tOkay, lips are soft too.\n",
    "106000\t107000\tSo I go on.\n",
    "107000\t108000\tOkay.\n",
    "108000\t109000\tOkay.\n",
    "109000\t110000\tOkay.\n",
    "110000\t111000\tOkay.\n",
    "111000\t112000\tAlright.\n",
    "112000\t113000\tThere's going on here.\n",
    "113000\t114000\tPlease.\n",
    "114000\t115000\tI'm trying.\n",
    "115000\t118000\tI can't believe what I'm saying.\n",
    "118000\t119000\tYou have really pretty lingerie.\n",
    "119000\t121000\tThat's not what I focused on first.\n",
    "121000\t123000\tI focused on two naked bodies.\n",
    "123000\t125000\tThat's what I focused on.\n",
    "125000\t130000\tDo you want us to put them over our clothes?\n",
    "130000\t140080\tif you're not inspect enough this week okay here's your step sisters this is\n",
    "140080\t146520\tbizarre I don't see you trying anything on I see the two of you naked embrace\n",
    "146520\t152720\tkissing each other and touching each other let me deploy to that it's it's you\n",
    "152720\t159320\tkeep telling us it's my luxury what are you doing those are my clothes you\n",
    "159320\t166680\ttry them all on and obviously took them all off but we were trying more on to\n",
    "166680\t172280\tbe fair you don't even make sense I don't see them on right now if you had\n",
    "172280\t176680\tcome like two minutes sooner really wouldn't make any difference you shouldn't\n",
    "176680\t182720\tbe going through my drawers picking up my private undergarments and wearing them\n",
    "182720\t187680\twe want to be sharing things that's what you said I'm telling her just like\n",
    "187680\t192360\ther with the ridiculous answers sharing things yeah you're gonna share\n",
    "192360\t200100\tsomething right now you know what you're gonna share you know the business\n",
    "200100\t207200\tdon't you my chair says what do we call that chair the spanking chair welcome\n",
    "207200\t212560\tto the household new step daughter can you do it like separately she doesn't have\n",
    "212560\t219080\tto see this oh yes she does I think she's seen enough I've seen enough to your\n",
    "219080\t224480\tfirst you're gonna show her how we do things around here not that it seems to\n",
    "224480\t245400\tmake much difference for me look at your bottom I have to spend you all week for\n",
    "245400\t274440\tthings you realize the gravity of what you've done\n",
    "274440\t282280\tthis is multi-level misbehavior it's not fun going through some of\n",
    "282280\t310360\tthose private things I don't buy that for a second I think you were having fun\n",
    "310360\t327280\tit was it was it was it was it was it was disrespectful it invades my\n",
    "327280\t350360\tprivacy this is your new step sister should invite her into our proper home\n",
    "350360\t379320\tnot going to battery you're such a bad influence\n",
    "379320\t400040\tyou see a brush there yeah hand it to her and you get back on the bed and you\n",
    "400040\t404520\twatch what's about to happen to you in a few minutes\n",
    "404520\t410160\tno too bad you want to go off you're the ones who took them off don't ask to put\n",
    "410160\t416360\tclothes on now just don't know what you want okay now you know what to do\n",
    "416360\t422040\tdon't you hand me the brush and you ask me for the spanking you deserve with that\n",
    "422040\t425560\thair brush\n",
    "425560\t440760\toh please give me the spanking I deserve with that hair brush\n",
    "440760\t452240\tI hope this stays in mind like that horrible image of the\n",
    "452240\t471960\ttalking heads home a bad image stay in mind\n",
    "471960\t473560\tI'm very so bad.\n",
    "473560\t474880\tI don't want to know.\n",
    "474880\t476360\tI'm going to know.\n",
    "476360\t478000\tI'm going to know.\n",
    "478000\t482000\tI'm going to know.\n",
    "482000\t483000\tPlease.\n",
    "483000\t485800\tIs this what it's?\n",
    "485800\t486800\tAnswer.\n",
    "486800\t489640\tIt's very cool, people.\n",
    "489640\t490960\tI need help.\n",
    "490960\t491960\tHelp.\n",
    "491960\t492960\tHelp.\n",
    "492960\t493960\tHelp.\n",
    "493960\t494960\tHelp.\n",
    "494960\t495960\tHelp.\n",
    "495960\t496960\tHelp.\n",
    "496960\t498960\tHelp.\n",
    "498960\t500960\tYou lift up a little.\n",
    "500960\t503960\tHelp me.\n",
    "503960\t505160\tEasy,osta.\n",
    "505160\t508160\tLike this.\n",
    "508160\t512160\tHelp.\n",
    "512160\t514160\tHelp me.\n",
    "514160\t516160\tHelp.\n",
    "516160\t519160\tHelp.\n",
    "519160\t519760\tHelp me.\n",
    "519760\t520760\tHelp me.\n",
    "520760\t521760\tHelp me.\n",
    "521760\t522760\tHelp me.\n",
    "522760\t524760\tHelp me.\n",
    "524760\t527600\tHelp.\n",
    "527600\t530400\tHelp me.\n",
    "530400\t530900\tHelp!\n",
    "532600\t533480\tHelp!\n",
    "534740\t536540\tHelp!\n",
    "544480\t545240\tHey.\n",
    "555160\t556460\tSo, were you unless?\n",
    "556540\t557180\tYeah!\n",
    "557280\t558780\tWere ever going to find the two of you here\n",
    "558780\t560380\tAnywhere like that in the house?\n",
    "560380\t561380\tNo, I promise.\n",
    "561380\t562180\tI promise.\n",
    "562180\t563180\tGood.\n",
    "563180\t564180\tGood.\n",
    "564180\t565180\tChase positions.\n",
    "571180\t573980\tYou're doing a little work.\n",
    "573980\t574980\tI'm going to get over my lap.\n",
    "574980\t584980\tThis is how we do things in this house.\n",
    "594980\t596980\tI don't know what's in store for you.\n",
    "596980\t603980\tDon't you?\n",
    "603980\t626780\tI hope you're going to learn how to behave in this house.\n",
    "626780\t629980\tAnd you're not going to be walking around with the red bottom leg.\n",
    "629980\t631980\tShe knows all the time, right?\n",
    "631980\t633980\tI miss my real mom.\n",
    "633980\t635980\tI'm your mom now.\n",
    "637980\t639980\tYou live here now.\n",
    "639980\t641980\tYou're stuck here.\n",
    "641980\t665980\tI can't do it here.\n",
    "665980\t671980\tIs that enough of the spiking I've given her?\n",
    "671980\t673980\tIs that enough of the spiking I've given her?\n",
    "673980\t675980\tShe doesn't deserve it.\n",
    "675980\t677980\tThat's what you do.\n",
    "677980\t679980\tThat's what you do.\n",
    "679980\t707980\tSit down for a moment.\n",
    "707980\t709980\tFour deer.\n",
    "709980\t711980\tFour deer.\n",
    "711980\t713980\tWe're not even halfway down.\n",
    "713980\t715980\tNo way.\n",
    "715980\t719980\tDon't get too out of the breath.\n",
    "727980\t731980\tWhere's that brush going?\n",
    "731980\t739980\tI need to hurt you.\n",
    "739980\t745980\tAnd you tell her what to ask me since you heard that phrase so many times.\n",
    "745980\t749980\tAsk her to give you the spiking.\n",
    "749980\t755980\tIs that enough of the spiking I've given her?\n",
    "755980\t761980\tPlease, boys, give me the spiking to decide what that brush is.\n",
    "761980\t763980\tCertainly what that is.\n",
    "763980\t767980\tOh, my God.\n",
    "767980\t769980\tOh!\n",
    "769980\t771980\tOh!\n",
    "771980\t773980\tOh!\n",
    "773980\t775980\tOh!\n",
    "775980\t777980\tOh!\n",
    "777980\t779980\tThere you see it all.\n",
    "779980\t781980\tHer first word's about.\n",
    "781980\t783980\tThis one's stuck in it.\n",
    "783980\t791980\tWhat the two of you were doing was really quite shocked.\n",
    "791980\t793980\tI'm sorry.\n",
    "793980\t795980\tI'm actually getting off lightly.\n",
    "795980\t797980\tNo!\n",
    "797980\t799980\tNo!\n",
    "799980\t801980\tNo!\n",
    "801980\t803980\tNo!\n",
    "803980\t805980\tNo!\n",
    "805980\t807980\tNo!\n",
    "807980\t809980\tNo!\n",
    "809980\t811980\tNo!\n",
    "811980\t813980\tI see you were scoring the chip.\n",
    "813980\t815980\tYou're getting the light treatment off of her.\n",
    "815980\t817980\tOver here.\n",
    "817980\t819980\tThere we go.\n",
    "819980\t821980\tThere we go.\n",
    "821980\t823980\tNo, it's funny yet.\n",
    "823980\t825980\tNo!\n",
    "825980\t827980\tOh!\n",
    "827980\t829980\tNo!\n",
    "829980\t831980\tNo!\n",
    "831980\t833980\tNo!\n",
    "833980\t835980\tNo!\n",
    "835980\t837980\tNo!\n",
    "837980\t839980\tNo!\n",
    "839980\t841980\tNo!\n",
    "841980\t843980\tNo!\n",
    "843980\t845980\tNo!\n",
    "845980\t847980\tNo!\n",
    "847980\t849980\tNo!\n",
    "849980\t851980\tNo!\n",
    "851980\t853980\tNo!\n",
    "853980\t855980\tNo!\n",
    "855980\t857980\tNo!\n",
    "857980\t859980\tYou're never going to poke if anybody else is drawing again.\n",
    "859980\t861980\tI'm so sorry.\n",
    "861980\t865980\tYou're ever going to try anybody's laserion again.\n",
    "865980\t869980\tAnd you're certainly never going to get naked with your skepticism again.\n",
    "869980\t871980\tAre you?\n",
    "871980\t873980\tShock it.\n",
    "873980\t875980\tNo!\n",
    "875980\t877980\tNo!\n",
    "877980\t879980\tNo!\n",
    "879980\t881980\tNo!\n",
    "881980\t883980\tNo!\n",
    "883980\t885980\tNo!\n",
    "885980\t887980\tNo!\n",
    "887980\t889980\tNo!\n",
    "889980\t891980\tNo!\n",
    "891980\t893980\tNo!\n",
    "893980\t895980\tNo!\n",
    "895980\t897980\tNo!\n",
    "897980\t899980\tNo!\n",
    "899980\t901980\tNo!\n",
    "901980\t903980\tNo!\n",
    "903980\t905980\tYes.\n",
    "905980\t909980\tI don't have my again.\n",
    "909980\t913980\tAnd you get, stand up there with her.\n",
    "913980\t915980\tYou get some closer sounds.\n",
    "915980\t921980\tYou can rub those red bottles of yours and then you're going to get.\n",
    "921980\t923980\tNo!\n",
    "923980\t925980\tNo!\n",
    "925980\t927980\tNo!\n",
    "927980\t929980\tNo!\n",
    "929980\t931980\tI'm really sorry.\n",
    "931980\t933980\tI didn't think this would happen.\n",
    "933980\t935980\tI was going to force you.\n",
    "935980\t937980\tI'm going to force you.\n",
    "937980\t941980\tYou're pretty scared.\n",
    "941980\t943980\tIt hurts.\n",
    "943980\t945980\tYou know.\n",
    "945980\t947980\tI mean, you're a bully.\n",
    "947980\t949980\tIt's got so many marks on you already.\n",
    "949980\t951980\tShe's evil.\n",
    "951980\t953980\tShe's a horrible mother.\n",
    "953980\t955980\tLiterally an evil stepmother.\n",
    "955980\t957980\tI thought your mom was your like.\n",
    "957980\t983980\tSo nice.\"\"\"]\n",
    "# sample_text = [\"strap on dildo fucking male strap on amateur forced feminization strap on bondage whipping caning dildo female domination cross dressing spanking humiliation sissy slut big tits MILF blonde BDSM i sissy training dildo blow job        \"]\n",
    "clean_sample_text = clean_text(sample_text[0])\n",
    "sample_vec = vec_saved.transform(sample_text)\n",
    "nb_saved.predict(sample_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc7771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
