{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime \n",
    "import multiprocessing\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selectolax.parser import HTMLParser\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from livelossplot.tf_keras import PlotLossesCallback\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict, Counter  # For word frequency\n",
    "\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import logging  # Setting up the loggings to monitor gensim\n",
    "# logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9744db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preparation\n",
    "data = pd.read_json('../datasets/dataset.json')\n",
    "mapping = {False: 0, True: 1}\n",
    "data.replace({'hasBadWords': mapping}, inplace=True)\n",
    "# data.hasBadWords = data.hasBadWords.apply(lambda x: 1 if x == True else 0)\n",
    "data.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "data.drop(['violation'], axis=1, inplace=True)\n",
    "data.shape\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "data = shuffle(data, random_state=RANDOM_STATE).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d951963",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22d3b6",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75899d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_text(text):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    text = bs(text,\"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens = text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\")+ ['14000kbps', 'november', '1080p', 'email', \n",
    "                                                 '4k', 'mp4', 'error', '404', '2022'])     \n",
    "    word_tokens = [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review = \" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0021f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:100].apply(lambda row: clean_text(row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_preprocessed'] = data.apply(lambda row: clean_text(row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13955f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text_preprocessed', 'labels', 'text']] # columns reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f91ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('../datasets/clear_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa66a6",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fc5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d45d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = counter_word(data.text_preprocessed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)\n",
    "oov_token = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter['deadly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in counter.most_common(50)]\n",
    "wd = WordCloud(background_color='white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93777ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(data.shape[0] * .8)\n",
    "\n",
    "train = data[:train_size]\n",
    "val = data[train_size:]\n",
    "\n",
    "# Split train and test\n",
    "\n",
    "X_train = train.text_preprocessed.to_numpy()\n",
    "y_train = train.labels.to_numpy()\n",
    "\n",
    "X_val = val.text_preprocessed.to_numpy()\n",
    "y_val = val.labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text_preprocessed.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63aea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6e616",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize a text corpus by turning each text into sentence of integers\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_unique_words) #, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(data.text_preprocessed.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # saving\n",
    "with open('../models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "# with open('../models/tokenizer.pickle', 'rb') as handle:\n",
    "#     tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dict()\n",
    "for word, number in tokenizer.word_index.items():\n",
    "    word_index[word] = number + 3\n",
    "\n",
    "word_index[\"<PAD>\"] = 0    \n",
    "word_index[\"<BEGINOFSEQUENCE>\"] = 1\n",
    "word_index[\"<UNKNOWN>\"] = 2\n",
    "\n",
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "\n",
    "\n",
    "# Последовательность индексов в текст\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "# Текст в последовательность индексов\n",
    "def encode(sequence):\n",
    "    words = sequence.lower().split()\n",
    "    words = [\"<BEGINOFSEQUENCE>\"] + words\n",
    "    idxs = [word_index.get(word, word_index[\"<UNKNOWN>\"]) for word in words]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['foot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d735498",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8705c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796d87a",
   "metadata": {},
   "source": [
    "# Find  vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c86922",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# max words in a sequence\n",
    "max_length = 200 #max([len(x) for x in X_train_sequences]) #256\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, \n",
    "                               value=word_index[\"<PAD>\"], \n",
    "                               maxlen=max_length, \n",
    "                               padding=\"post\", \n",
    "                               truncating=\"post\")\n",
    "X_val_padded = pad_sequences(X_val_sequences, \n",
    "                             value=word_index[\"<PAD>\"], \n",
    "                             maxlen=max_length, \n",
    "                             padding=\"post\", \n",
    "                             truncating=\"post\")\n",
    "\n",
    "X_train_padded.shape, X_val_padded.shape\n",
    "\n",
    "print(f\"Length examples: {[len(X_train_padded[0]), len(X_train_padded[1])]}\")\n",
    "print('=' * 50)\n",
    "print(f\"Entry example: {X_train_padded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219467b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0004bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(X_train_sequences[10]))\n",
    "print(encode(decode(X_train_sequences[10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c29ea8",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create LSTM model\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# # Word embeddings give us a way to use an efficient, dense representation in which similar words have\n",
    "# # a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# # dense vector of floating point values (the length of the vector is a parameter you specify)\n",
    "\n",
    "max_length = 200 \n",
    "# embedding_vector_features=45 # 32\n",
    "\n",
    "# # \n",
    "# # tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(layers.Embedding(num_unique_words, embedding_vector_features, input_length=max_length))\n",
    "\n",
    "# # The layer will take as input as integer matrix of size (batcg, input_length)\n",
    "# # and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# # Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "# model.add(layers.LSTM(64, dropout=.10)) # dropout=.1\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4018253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "\n",
    "# embedding_vector_features=45\n",
    "\n",
    "# model=keras.models.Sequential()\n",
    "\n",
    "# model.add(layers.Embedding(num_unique_words, embedding_vector_features,input_length=max_length))\n",
    "\n",
    "# model.add(layers.LSTM(128, return_sequences=True))#dropout=.10\n",
    " \n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layers.LSTM(128,activation='relu'))\n",
    "\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# # for units in [128,128,64,32]:\n",
    "\n",
    "# # model.add(Dense(units,activation='relu'))\n",
    "\n",
    "# # model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(layers.Dense(32,activation='relu'))\n",
    "\n",
    "# model.add(layers.Dropout(0.2))\n",
    "\n",
    "# model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "# #####Compile\n",
    "# loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# optimizer = keras.optimizers.legacy.Adam(learning_rate=.01)\n",
    "# metrics = ['accuracy']\n",
    "# # metrics = [tf.keras.metrics.BinaryAccuracy(),\n",
    "# #            tf.keras.metrics.FalseNegatives(),\n",
    "# #            tf.keras.metrics.FalsePositives(),\n",
    "# #           ]\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da58a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    Embedding(num_unique_words, 32, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        LSTM(64, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)),\n",
    "    tf.keras.layers.Bidirectional(    \n",
    "        LSTM(64, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(8),\n",
    "#     Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=.01)\n",
    "metrics = ['accuracy']\n",
    "# metrics = [tf.keras.metrics.BinaryAccuracy(),\n",
    "#            tf.keras.metrics.FalseNegatives(),\n",
    "#            tf.keras.metrics.FalsePositives(),\n",
    "#           ]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c1d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw model\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6eecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path = Path('../models/bad_words')\n",
    "path.mkdir(exist_ok=True) \n",
    "cpt_filename = '{epoch:02d}_checkpoint_{val_loss:.2f}.hdf5'\n",
    "cpt_path = str(path / cpt_filename)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(cpt_path, monitor='val_loss', verbose=1, \n",
    "                                                save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=.01)\n",
    "metrics = ['accuracy']\n",
    "# metrics = [tf.keras.metrics.BinaryAccuracy(),\n",
    "#            tf.keras.metrics.FalseNegatives(),\n",
    "#            tf.keras.metrics.FalsePositives(),\n",
    "#           ]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_val_padded, y_val), verbose=1, \n",
    "          callbacks=[checkpoint, PlotLossesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(model.history.history)\n",
    "history_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_df.loss)\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('Количество ошибок');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_df.accuracy,\n",
    "         label='Доля верных ответов на обучающей выборке')\n",
    "plt.plot(history_df.val_accuracy,\n",
    "         label='Доля верных ответов на валидационной выборке')\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('Доля верных ответов')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddlayers_matrix = model.layers[0].get_weights()[0]\n",
    "embeddlayers_matrix[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951972f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'sex'\n",
    "word_number = word_ind[word]\n",
    "print('Номер слова', word_number)\n",
    "print('Вектор для слова', embeddlayers_matrix[word_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbba7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../datasets/embeds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcdbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvi = dict()\n",
    "\n",
    "for key, value in word_ind.items():\n",
    "    rvi[value] = key\n",
    "rvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b56377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(filename, 'w') as f:\n",
    "#     for word_num in range(num_unique_words):\n",
    "#         word = rvi[word_num]\n",
    "#         vec = embeddlayers_matrix[word_num]\n",
    "#         f.write(word + ',')\n",
    "#         f.write(','.join([str(x) for x in vec]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n20 $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvi[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '../models/bad_words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0686509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/bad_words/04_checkpoint_0.05.hdf5') # loading weights - model had created erlier\n",
    "loss, acc = model.evaluate(X_val_padded, y_val)\n",
    "print(f'Accuracy of restored model {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.models.load_model('../models/bad_words/04_checkpoint_0.05.hdf5')\n",
    "# base_model.trainable = False # замораживаем базовую модель\n",
    "\n",
    "loss, acc = base_model.evaluate(X_val_padded, y_val)\n",
    "print(f'Accuracy of restored model {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_train_padded)\n",
    "predictions = [1 if p > .5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded[1200:1210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[1200:1210])\n",
    "print(y_train[1200:1210])\n",
    "print(predictions[1200:1210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95589f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predictions'] = predictions\n",
    "train = train[['text_preprocessed', 'labels', 'predictions', 'text']] # columns reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb366b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59591589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../datasets/wo_html.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('../models/bad_words/modelSequential_HTML_final.h5')\n",
    "# model = keras.models.load_model('../models/modelSequential_HTML_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_new = keras.models.load_model('../models/modelSequential_wo_HTML.h5')\n",
    "# model_new = keras.models.load_model('../models/bad_words/08_checkpoint_0.07.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../datasets/sub.txt', 'r') as file:\n",
    "#     raw_text = ''.join(file.readlines())\n",
    "\n",
    "# raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bf1d6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_train_data():\n",
    "    data = pd.read_json('../datasets/dataset.json')\n",
    "    mapping = {False: 0, True: 1}\n",
    "    data.replace({'hasBadWords': mapping}, inplace=True)\n",
    "    data.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "    data.rename(columns={\"text\": \"raw_text\"}, inplace=True)\n",
    "    data.drop(['violation'], axis=1, inplace=True)\n",
    "    print('Data size %d' % len(data))\n",
    "    print('Data headers %s' % data.columns.values)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv('../datasets/BadWordsService_ml_has_7days.csv')\n",
    "df_neg =  pd.read_csv('../datasets/BadWordsService_ml_has_no_7days.csv')\n",
    "df_val = pd.read_csv('../datasets/BadWordsService_ml_has_today.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82452b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.drop('@timestamp', axis=1, inplace=True)\n",
    "df_neg.drop('@timestamp', axis=1, inplace=True)\n",
    "df_val.drop('@timestamp', axis=1, inplace=True)\n",
    "df_val['ctxt_.time_spent'] = df_val['ctxt_.time_spent'] * 1000 # convert seconds to milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.rename(columns={\"ctxt_.text\": \"raw_text\", \"ctxt_.time_spent\": \"time_spent_ms\"}, inplace=True)\n",
    "df_neg.rename(columns={\"ctxt_.text\": \"raw_text\", \"ctxt_.time_spent\": \"time_spent_ms\"}, inplace=True)\n",
    "df_val.rename(columns={\"ctxt_.text\": \"raw_text\"}, inplace=True)\n",
    "df_val.rename(columns={\"ctxt_.time_spent\": \"time_spent_ms\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d34046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['labels'] = 1\n",
    "df_neg['labels'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f686ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values_neg = df_neg[df_neg.isna().any(axis=1)]\n",
    "print(len(nan_values_neg))\n",
    "\n",
    "# nan_values_pos = df_pos[df_pos.isna().any(axis=1)]\n",
    "# print(len(nan_values_pos))\n",
    "\n",
    "nan_values = df_val[df_val.isna().any(axis=1)]\n",
    "print(len(nan_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg.dropna(inplace=True)\n",
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f061f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos['text'] = df_pos.apply(lambda row: clean_text(row.raw_text), axis=1)\n",
    "df_neg['text'] = df_neg.apply(lambda row: clean_text(row.raw_text), axis=1)\n",
    "df_val['text'] = df_val.apply(lambda row: clean_text(row.raw_text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99aa7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59beb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.drop('raw_text', axis=1, inplace=True)\n",
    "df_neg.drop('raw_text', axis=1, inplace=True)\n",
    "df_val.drop('raw_text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7cd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.shape, df_neg.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df_tra = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "df_tra = shuffle(df_tra, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "df_tra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e53c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra = df_tra[['text', 'labels']]#, 'raw_text']] # columns reorder\n",
    "df_tra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4082eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in counter.most_common(50)]\n",
    "wd = WordCloud(background_color='white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69422854",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_size = int(df_tra.shape[0] * .8)\n",
    "\n",
    "train = df_tra[:train_size]\n",
    "val = df_tra[train_size:]\n",
    "\n",
    "# Split train and test\n",
    "\n",
    "X_train = train.text.to_numpy()\n",
    "y_train = train.labels.to_numpy()\n",
    "\n",
    "X_val = val.text.to_numpy()\n",
    "y_val = val.labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16907ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # loading\n",
    "with open('../models/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize a text corpus by turning each text into sentence of integers\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=num_unique_words) #, oov_token=oov_token)\n",
    "# tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fad97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word have a unique index\n",
    "word_index = tokenizer.word_index\n",
    "word_index['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99561fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8350af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[1])\n",
    "print(X_train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd36f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# max words in a sequence\n",
    "max_length = 200 #max([len(x) for x in X_train_sequences]) #256\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "X_train_padded.shape, X_val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[10])\n",
    "print(X_train_sequences[10])\n",
    "print(X_train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db4e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# model = keras.models.Sequential([model,\n",
    "#                                  layers.Embedding(num_unique_words, 32, input_length=max_length),\n",
    "#                                  layers.LSTM(64, dropout=.10),\n",
    "#                                  layers.Dense(1, activation='sigmoid')\n",
    "#                                 ])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    Embedding(num_unique_words, 32, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        LSTM(64, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)),\n",
    "    tf.keras.layers.Bidirectional(    \n",
    "        LSTM(64, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(8),\n",
    "#     Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=.001)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "model.fit(X_train_padded, y_train, epochs=25, batch_size=64, validation_data=(X_val_padded, y_val), verbose=1, \n",
    "          callbacks=[checkpoint])\n",
    "\n",
    "# model.fit(X_train_padded, y_train, epochs=15, validation_data=(X_val_padded, y_val), verbose=1, \n",
    "#           callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(model.history.history)\n",
    "history_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b454fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_df.loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a56eba",
   "metadata": {},
   "source": [
    "# Check sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe408738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from datetime import datetime \n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "# pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "######## new\n",
    "import itertools as itt\n",
    "chunksize = 200\n",
    "    \n",
    "# vectorize a text corpus by turning each text into sentence of integers\n",
    "max_len = 200\n",
    "# tokenizer = Tokenizer(num_words=num_unique_words, oov_token=oov_token)\n",
    "# tokenizer.fit_on_texts(X)\n",
    "\n",
    "\n",
    "values = []\n",
    "columns = ['text', 'time_elapsed_ms','label']\n",
    "for index, elem in enumerate(df_val.text):\n",
    "    start_time = datetime.now() \n",
    "    cnt = itt.count()\n",
    "    test_text_np = np.array([''.join(grp) for k, grp in itt.groupby(elem, key=lambda x: next(cnt)//chunksize%2)])\n",
    "    tokenizer.fit_on_texts(test_text_np)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_text_np)   \n",
    "    test_sequences_padded = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "#     print(test_text_np)\n",
    "#     print('LEN', len(test_text_np))\n",
    "#     print('test_sequences', test_sequences)\n",
    "#     print('test_sequences_padded', test_sequences_padded)\n",
    "    \n",
    "    # for word in test_text_preprocessed:\n",
    "    # Get max training sequence length\n",
    "  \n",
    "\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    predictions = model.predict(test_sequences_padded)\n",
    "    predictions = [1 if p > .5 else 0 for p in predictions]\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "\n",
    "#     print('Time elapsed (ms) {}'.format(time_elapsed.microseconds/1000))\n",
    "#     print('Prediction:', max(predictions))\n",
    "    if elem not in values:\n",
    "#         print('**************', index, '~~~~~~~~~~~', elem, values[index-1])\n",
    "        values.append([elem, round(time_elapsed.microseconds/1000, 1), max(predictions)])\n",
    "        values[index][1] += round(time_elapsed.microseconds/1000, 1)\n",
    "#         print('**************', index, '~~~~~~~~~~~', elem, '################', values[index][1])\n",
    "df_result = pd.DataFrame(data=values, columns=columns)\n",
    "########\n",
    "# test_text_np = np.array([test_text_preprocessed])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_text_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(df_result)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635773cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06952b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_united = df_val.merge(df_result, right_on='text', left_on='text')\n",
    "df_united['time_delta'] = df_united.time_spent_ms - df_united.time_elapsed_ms\n",
    "df_united = df_united[['text', 'time_spent_ms', 'time_elapsed_ms', 'time_delta', 'label']]\n",
    "df_united.drop_duplicates(['text'], keep='last', ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42485e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_united.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619349bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_united.to_csv('../datasets/last_pred_old.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## new\n",
    "import itertools as itt\n",
    "chunksize = 200\n",
    "max_len = chunksize\n",
    "\n",
    "# raw_text = \"After a morning of horse-back riding, Sara Domina returns home with her boots dirtied, the soles soiled with .....Her slave is in waiting, unawares of what is about to happen to him .....\"\n",
    "raw_text = \"\"\"nymphomaniac forced submission wmv\"\"\"\n",
    "\n",
    "test_text_preprocessed = [clean_text(raw_text)]\n",
    "# print(test_text_preprocessed)\n",
    "values = []\n",
    "columns = ['text', 'time_elapsed_ms','label']\n",
    "for index, elem in enumerate(test_text_preprocessed):\n",
    "    start_time = datetime.now() \n",
    "    cnt = itt.count()\n",
    "    test_text_np = np.array([''.join(grp) for k, grp in itt.groupby(elem, key=lambda x: next(cnt)//chunksize%2)])\n",
    "    tokenizer.fit_on_texts(test_text_np)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_text_np)   \n",
    "    test_sequences_padded = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "#     print(test_text_np)\n",
    "#     print('LEN', len(test_text_np))\n",
    "#     print('test_sequences', test_sequences)\n",
    "#     print('test_sequences_padded', test_sequences_padded)\n",
    "    \n",
    "    # for word in test_text_preprocessed:\n",
    "    # Get max training sequence length\n",
    "  \n",
    "\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    predictions = model.predict(test_sequences_padded)\n",
    "    predictions = [1 if p > .5 else 0 for p in predictions]\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time \n",
    "\n",
    "#     print('Time elapsed (ms) {}'.format(time_elapsed.microseconds/1000))\n",
    "#     print('Prediction:', max(predictions))\n",
    "    if elem not in values:\n",
    "#         print('**************', index, '~~~~~~~~~~~', elem, values[index-1])\n",
    "        values.append([elem, round(time_elapsed.microseconds/1000, 1), max(predictions)])\n",
    "        values[index][1] += round(time_elapsed.microseconds/1000, 1)\n",
    "#         print('**************', index, '~~~~~~~~~~~', elem, '################', values[index][1])\n",
    "df_result = pd.DataFrame(data=values, columns=columns)\n",
    "df_result\n",
    "# ####### new\n",
    "# myseq = raw_text\n",
    "# cnt = itt.count()\n",
    "# test_text_np = np.array([''.join(grp) for k,grp in itt.groupby(myseq, key=lambda x: next(cnt)//chunksize%2)])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_text_np)\n",
    "# #######\n",
    "\n",
    "# test_text_np = np.array([test_text_preprocessed])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_text_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in test_text_preprocessed:\n",
    "# Get max training sequence length\n",
    "max_len = 200\n",
    "\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e99d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_text_np)\n",
    "print(test_sequences)\n",
    "print(test_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eddbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now() \n",
    "\n",
    "predictions = model.predict(test_sequences_padded)\n",
    "predictions = [1 if p > .5 else 0 for p in predictions]\n",
    "\n",
    "time_elapsed = datetime.now() - start_time \n",
    "\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c848a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb988af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# predictions = model_new.predict(test_sequences_padded)\n",
    "# predictions = [1 if p > .5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19523288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
