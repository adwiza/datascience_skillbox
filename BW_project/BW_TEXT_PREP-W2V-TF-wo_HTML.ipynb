{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484c5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selectolax.parser import HTMLParser\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict, Counter  # For word frequency\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# import logging  # Setting up the loggings to monitor gensim\n",
    "# logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9744db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preparation\n",
    "data = pd.read_json('../datasets/dataset.json')\n",
    "mapping = {False: 0, True: 1}\n",
    "data.replace({'hasBadWords': mapping}, inplace=True)\n",
    "# data.hasBadWords = data.hasBadWords.apply(lambda x: 1 if x == True else 0)\n",
    "data.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "data.drop(['violation'], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d951963",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22d3b6",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27238a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text, stop_words, punctuation_marks): #, morph):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    preprocessed_text = []\n",
    "    for token in tokens:\n",
    "        if token not in punctuation_marks:\n",
    "            lemma = token #morph.parse(token)[0].normal_form\n",
    "            if lemma not in stop_words:\n",
    "                preprocessed_text.append(lemma)\n",
    "    return ' '.join(preprocessed_text)\n",
    "\n",
    "punctuation_marks = ['!', ',', ';', ':', '(', ')', '-', '--', '?', '@', '....', '~',\n",
    "                     '.', '..', '...', '....................', '<', '>', '=', '»', '|', '’', '`', '+', '$',\n",
    "                     '&', '#', '+++', '*', '``', '%', '[', ']', '{', '}', '√©']\n",
    "\n",
    "stop_words = stopwords.words('english') + ['14000kbps', 'november', '1080p', '4k', 'mp4', 'error', '404', '2022']\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39168897",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Replace ips\n",
    "    s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
    "    # Isolate punctuation\n",
    "    s = re.sub(r'([.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Replace numbers and symbols with language\n",
    "    s = s.replace('&', ' and ')\n",
    "    s = s.replace('@', ' at ')\n",
    "    s = s.replace('0', ' zero ')\n",
    "    s = s.replace('1', ' one ')\n",
    "    s = s.replace('2', ' two ')\n",
    "    s = s.replace('3', ' three ')\n",
    "    s = s.replace('4', ' four ')\n",
    "    s = s.replace('5', ' five ')\n",
    "    s = s.replace('6', ' six ')\n",
    "    s = s.replace('7', ' seven ')\n",
    "    s = s.replace('8', ' eight ')\n",
    "    s = s.replace('9', ' nine ')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:1000].apply(lambda row: str(row.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:100].apply(lambda row: bs(row['text']).get_text().replace('\\n',' '),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0021f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:100].apply(lambda row: preprocess(row.text, punctuation_marks, stop_words), axis=1) #, morph), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the rows with \"<div\"\n",
    "# identify partial string\n",
    "discard = [\"<div \", \"<p \", \"<span \", \"<p>\", \"<div>\", \"<h\", \"<input \", \"center>\", \"<a \", \n",
    "           \"<td>\", \"<\", \">\", r\"              \", \"Ø\", '√ú', 'http://']\n",
    "  \n",
    "data = data[~data.text.str.contains('|'.join(discard))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda row: bs(row.text, 'lxml').get_text().replace('\\r\\n', ' ').replace('/', ' ').replace('\"', '\\\"'), axis=1)\n",
    "# data['text'] = data.apply(lambda row: HTMLParser(row.text).body.text(separator=' ').replace('\\r\\n',' '),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_preprocessed'] = data.apply(lambda row: preprocess(row.text, punctuation_marks, stop_words), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13955f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text_preprocessed', 'labels', 'text']] # columns reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f91ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('../datasets/clear_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa66a6",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fc5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d45d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = counter_word(data.text_preprocessed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b3542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)\n",
    "oov_token = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import *\n",
    "word_freq = [i for i in counter.most_common(50)]\n",
    "wd = WordCloud(background_color='white')\n",
    "wd.generate_from_frequencies(dict(word_freq))\n",
    "plt.figure()\n",
    "plt.imshow(wd, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93777ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(data.shape[0] * .8)\n",
    "\n",
    "train = data[:train_size]\n",
    "val = data[train_size:]\n",
    "\n",
    "# Split train and test\n",
    "\n",
    "X_train = train.text_preprocessed.to_numpy()\n",
    "y_train = train.labels.to_numpy()\n",
    "\n",
    "X_val = val.text_preprocessed.to_numpy()\n",
    "y_val = val.labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.text_preprocessed.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63aea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6e616",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize a text corpus by turning each text into sentence of integers\n",
    "\n",
    "tokenizer2 = Tokenizer(num_words=num_unique_words, oov_token=oov_token)\n",
    "tokenizer2.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('../models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('../models/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fa1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word have a unique index\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c86922",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[1])\n",
    "print(X_train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da83e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# max words in a sequence\n",
    "max_length = max([len(x) for x in X_train_sequences]) #256 #20\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "X_train_padded.shape, X_val_padded.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec44ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[10])\n",
    "print(X_train_sequences[10])\n",
    "print(X_train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8705c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bd7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f52afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "decoded_text = decode(X_train_sequences[10])\n",
    "\n",
    "print(X_train_sequences[10])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b0700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have\n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify)\n",
    "\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "# The layer will take as input as integer matrix of size (batcg, input_length)\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=.1))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6eecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('../models/bad_words.model')\n",
    "path.mkdir(exist_ok=True) \n",
    "cpt_filename = '{epoch:02d}_checkpoint_{val_loss:.2f}.hdf5'\n",
    "cpt_path = str(path / cpt_filename)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(cpt_path, monitor='val_loss', verbose=1, \n",
    "                                                save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=.001)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd79bedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_padded, y_train, epochs=15, validation_data=(X_val_padded, y_val), verbose=1, \n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c29bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(model.history.history)\n",
    "history_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history_df.loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0686509",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../models/bad_words.model/04_checkpoint_0.04.hdf5') # loading weights - model had created erlier\n",
    "loss, acc = model.evaluate(X_val_padded, y_val)\n",
    "print(f'Accuracy of restored model {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../models/bad_words.model/04_checkpoint_0.04.hdf5')\n",
    "loss, acc = model.evaluate(X_val_padded, y_val)\n",
    "print(f'Accuracy of restored model {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '../models/bad_words.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_train_padded)\n",
    "predictions = [1 if p > .5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[2000:2010])\n",
    "print(y_train[2000:2010])\n",
    "print(predictions[2000:2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95589f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predictions'] = predictions\n",
    "train = train[['text_preprocessed', 'labels', 'predictions', 'text']] # columns reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb366b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59591589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../datasets/wo_html.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/modelSequential_wo_HTML.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_new = keras.models.load_model('../models/modelSequential_wo_HTML.h5')\n",
    "model_new = keras.models.load_model('../models/bad_words.model/04_checkpoint_0.04.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ba6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_new.predict(X_train_padded)\n",
    "predictions = [1 if p > .5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a56eba",
   "metadata": {},
   "source": [
    "# Check sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe408738",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Welcome to Jandro The Tickler. What \\\n",
    "          you're seeing here is completely real! So here's the premise: Husbands, \\\n",
    "          boyfriends, friends, etc, hire me, \\\"The Tickler\\\", to show \\\n",
    "          up at a specified location at a specific time with one mission: Find \\\n",
    "          the girl, tie her up, surprise her, and tickle the hell out of her! \\\n",
    "          Sometimes the girls are in the know, and sometimes they're not:) The \\\n",
    "          bonds are real, the reactions are certainly real, and the tickle tools \\\n",
    "          are 100% real. The end result is usually a surprised, shocked, tortured, \\\n",
    "          turned on, worn out girl, with hardly the strength to wave at the camera \\\n",
    "          before I exit:) I basically wanted to combine Tickle Torture with Foot, \\\n",
    "          Sleepy, Voyeur, Light Bondage, and even Forced O. The premise allows \\\n",
    "          all of these to take place per vid.\"\n",
    "test_text_preprocessed = bs(raw_text, 'lxml').get_text().replace('\\r\\n',' ')\n",
    "test_text_preprocessed = preprocess(test_text_preprocessed, punctuation_marks, stop_words)\n",
    "test_text_np = np.array([test_text_preprocessed])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_text_preprocessed)\n",
    "print(test_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in test_text_preprocessed:\n",
    "# Get max training sequence length\n",
    "max_len = max([len(x) for x in test_sequences])\n",
    "\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e99d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_text_preprocessed)\n",
    "print(test_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eddbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = model_new.predict(test_sequences_padded)\n",
    "predictions = [1 if p > .5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da377a5f",
   "metadata": {
    "code_folding": [
     12
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict, Counter  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict, Counter  # For word frequency\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess(text, stop_words, punctuation_marks): #, morph):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    preprocessed_text = []\n",
    "    for token in tokens:\n",
    "        if token not in punctuation_marks:\n",
    "            lemma = token #morph.parse(token)[0].normal_form\n",
    "            if lemma not in stop_words:\n",
    "                preprocessed_text.append(lemma)\n",
    "    return ' '.join(preprocessed_text)\n",
    "\n",
    "punctuation_marks = ['!', ',', ';', ':', '(', ')', '-', '--', '?', '@', '....', '~', '¬ß'\n",
    "                     '.', '..', '...', '....................', '<', '>', '=', '»', '|', '’', '`', '+', '$',\n",
    "                     '&', '#', '+++', '*', '``', '%', '[', ']', '{', '}', '√©', '√™', '¬†', '√¢']\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english') + ['14000kbps', 'https', \"'s\", \"'m\", 'http', 'mp4', 'error', '404',\n",
    "                                          'Error 404', '404error']\n",
    "\n",
    "# loading\n",
    "with open('../models/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "model = keras.models.load_model('../models/modelSequential_wo_HTML.h5')\n",
    "\n",
    "raw_text = 'nigger'\n",
    "\n",
    "\n",
    "def predict(sequences):\n",
    "    sequences_padded = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "    predictions = model.predict(sequences_padded)\n",
    "    predictions = [1 if p > .5 else 0 for p in predictions]\n",
    "    return predictions\n",
    "\n",
    "# Data loading and preparation\n",
    "data = pd.read_json('../datasets/neil_ProducerClipSite_rand.json')\n",
    "mapping = {False: 0, True: 1}\n",
    "# data.replace({'hasBadWords': mapping}, inplace=True)\n",
    "# data.hasBadWords = data.hasBadWords.apply(lambda x: 1 if x == True else 0)\n",
    "# data.rename(columns={\"hasBadWords\": \"labels\"}, inplace=True)\n",
    "data.drop(['SiteID', 'Title', 'Description', 'Keywords', 'Bottom'], axis=1, inplace=True)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d22351",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data['text'] = data.apply(lambda row: bs(row.Top, 'lxml').get_text().replace('\\r\\n',' ').replace('.', ''), axis=1)\n",
    "data['text_preprocessed'] = data.apply(lambda row: preprocess(row.text, punctuation_marks, stop_words), axis=1)\n",
    "\n",
    "# Get max training sequence length\n",
    "max_length = max([len(x) for x in X_val_sequences])\n",
    "\n",
    "X_val = data.text_preprocessed.to_numpy()\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=256, padding=\"post\", truncating=\"post\")\n",
    "# X_val_padded = pad_sequences(X_val_sequences, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "\n",
    "predictions = model.predict(X_val_padded)\n",
    "predictions = [1 if p > .5 else 0 for p in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7766407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_val_padded[100:102]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3087",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892604",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Top', 'predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a04a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../datasets/last_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26191e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('../datasets/last_one.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac7a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
