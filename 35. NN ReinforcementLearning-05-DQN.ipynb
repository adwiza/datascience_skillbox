{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb07890c",
   "metadata": {},
   "source": [
    "# Реализация DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfeb1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00199f9d",
   "metadata": {},
   "source": [
    "# Создание игровой среды\n",
    "\n",
    "Создадим симулятор Frozen Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd050735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 16\n",
      "Actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "NUM_STATES = env.observation_space.n\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "print('States: {}'.format(NUM_STATES))\n",
    "print('Actions: {}'.format(NUM_ACTIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89814e56",
   "metadata": {},
   "source": [
    "# Параметры обучения\n",
    "\n",
    "* lr - скорость обучения (в теории это был параметр alpha)\n",
    "* gamma - параметр дисконтирования\n",
    "* NUM_EPISODES - сколько всего эпизодов игры будем осуществлять\n",
    "* MAX_STEPS - максимальное количество шагов в рамках одного эпизода. Эпизод может закончится раньше. Это ограничение\n",
    "нужно, если агент зашел в какой-то тупик и там застрял. Или для бесконечных игр/симуляций.\n",
    "\n",
    "Кроме того, будем смотреть на награду, усредненную в некотором временнОм окне (окно по эпизодам).\n",
    "* REWARD_AWERAGE_WINDOW - размер этого окна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df84db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .1 # learning rate\n",
    "gamma = .99 # параметр дисконтирования\n",
    "\n",
    "NUM_EPISODES = 1000 # число эпизодов для обучения\n",
    "MAX_STEPS = 100 # максимальное число шагов в эпизоде\n",
    "\n",
    "REWARD_AVERAGE_WINDOW = 20 # окно для усреднения наград по эпизодам"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8264dce",
   "metadata": {},
   "source": [
    "# Создание модели\n",
    "Создадим простую модель для аппроксимации оптимальной Q-функции. Формально это будет нейронная сеть на Tensorflow.\n",
    "Однако по факту, мы будем использовать лишь один Embedding слой. Это обучаемый слой, который по некоторому целочисленному\n",
    "входу выдает некоторый вектор. Именно это нам и нужно для отображения состояния s в вектор Q(s, :) для различных действий.\n",
    "Интересным фактом является то, что так как Embedding слой по сути матрица а прямое распостранение для него -- это выбор\n",
    "s-той строчки в этой матрице, такая модель эквивалентна обычной Q-таблице. Такое сработает лишь потому, что у нас очень простая\n",
    "задача (Frozen Lake). Для более сложных задач надо использовать более сложные нейросети.\n",
    "Кроме самой модели создадим функцию для инференса evalQ, которая будет вычислять вектор Q(s, :) по входному состоянию s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a340359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(NUM_STATES, NUM_ACTIONS, tf.initializers.RandomUniform(0, 1)),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "])\n",
    "\n",
    "def evalQ(s):\n",
    "    inp = np.array([[s]], dtype=np.int32)\n",
    "    return model(inp).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56e04f",
   "metadata": {},
   "source": [
    "# Подготовка к обучению\n",
    "В качестве функции потерь будем использовать квадрат нормы разницы между целевым и предсказанным вектором Q(s, :), как это обычно\n",
    "делается в задачах регрессии (ведь нам нужно притянуть друг к другу эти два вектора). Зададим это функцией loss()\n",
    "В качестве оптимизатора для нейросети будем использовать SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec36399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(q1, q2):\n",
    "    return tf.reduce_sum(tf.square(q1 - q2))\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr)\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2d338",
   "metadata": {},
   "source": [
    "# Обучение DQN\n",
    "Обучение DQN во многом похоже на табличный Q-Learning. Рассмотрим подробнее лишь те части, которвые отличаются.\n",
    "Чтобы получить значения Q-функции для текущего состояния s и всех действий, поспользуемся фунукцией evalQ:\n",
    "    Q_s = evalQ(s)\n",
    "    \n",
    " Во время обучения будем использовать eps-greedy подход для выбора действия (в зависимости от параметра eps выбирать\n",
    " случайное действие по текущей политике).\n",
    " Самое интересное - как сделать обновление Q-функции. Для начала, надо разобраться, как получить целевой вектор Q-target.\n",
    " Вспомним, что в Q-Learning мы должны обновить толькл значение Q(s, a) (через уравление Беллмана) Но так как нейросеть\n",
    " предсказывает вектор для всех возможных действий (Q(s, :)), сделать нужно следующим образом.\n",
    " \n",
    " Пусть целевой вектор для Q(s, :) будет равен исходным (предсказанным нейросетью) значениям для всех действий, кроме действия a.\n",
    " То есть скопируем в Q_target текущий предсказанный Q_s и заменим в нем лишь Q_target[a] на то, что дает нам Беллман. И\n",
    " Теперь наша задача \"обучить\" нейросеть на этот целевой вектор, то есть заставить её изменить свои веса так, чтобы толок значение\n",
    " для действия a изменилось, а остальные значения Q(s, :) по возможности не изменялись. Делаем это через минимизацию ошибки между\n",
    " этими векторами.\n",
    " А чтобы сделать один шаг минимизации ошибки,  нам надо сделать один шаг градиентного спуска (или в общем случае один шаг\n",
    " обучения модели). Сделать это можно с помощью model.train_on_batch(...), которая делает шаг обучения на данном\n",
    " батче. На входе у нее батч входов и батч правильных ответов. У нас будет батч из одного элемента.\n",
    " В остальном все тоже самое, как было в Q-learning.\n",
    "     И еще дополнительно накапливаем список усредненных наград -- totalRewardAverageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5dba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 12:18:56.493911: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "pathLenList = [] # длины траекторий по эпизодам\n",
    "totalRewardList = [] # суммарные награды по эпизодам\n",
    "totalRewardAverageList = [] # суммарные награды по эпизодам (среднее по окну)\n",
    "\n",
    "for i in range(NUM_EPISODES):\n",
    "\n",
    "    eps = 1.0 - i / NUM_EPISODES\n",
    "\n",
    "    s = env.reset()[0] #[1]['prob']\n",
    "\n",
    "    totalReward = 0\n",
    "    step = 0\n",
    "\n",
    "    while step < MAX_STEPS:\n",
    "        step += 1\n",
    "\n",
    "        Q_s = evalQ(s)\n",
    "        \n",
    "        if np.random.rand() < eps:\n",
    "            # Выбор случайного действия\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            # Выбор действия по текущей политике\n",
    "            a = np.argmax(Q_s)\n",
    "        \n",
    "        # Сделать шаг\n",
    "        s1, r, _, done = env.step(a)[:4]\n",
    "\n",
    "        Q_s1 = evalQ(s1)\n",
    "        \n",
    "        # Новое (целевое) значение Q-функции\n",
    "        Q_target = Q_s\n",
    "        if done:\n",
    "            Q_target[a] = r\n",
    "        else:\n",
    "            Q_target[a] = r + gamma * np.max(Q_s1)\n",
    "              \n",
    "        # Обновление Q-функции\n",
    "        inp = np.array([[s]], dtype=np.int32)\n",
    "        model.train_on_batch(inp, Q_target[None, None, ...])\n",
    "        \n",
    "        totalReward += r\n",
    "        s = s1\n",
    "        \n",
    "        # Если конец эпизода\n",
    "        if done:\n",
    "            break\n",
    "    pathLenList.append(step)\n",
    "    totalRewardList.append(totalReward)\n",
    "    \n",
    "    if i % REWARD_AVERAGE_WINDOW == 0 and i >= REWARD_AVERAGE_WINDOW:\n",
    "        totalRewardAverage = np.mean(totalRewardList[-REWARD_AVERAGE_WINDOW:])\n",
    "        totalRewardAverageList.append(totalRewardAverage)\n",
    "        if i % 100 == 0:\n",
    "            print('Episode {}: average total reward = {}'.format(i, totalRewardAverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f67ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pathLenList)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f24c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(totalRewardAverageList)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b17bdd",
   "metadata": {},
   "source": [
    "# Запуск симуляции\n",
    "Запустим симуляцию для Frozen Lake так же, как мы это делали до этого.\n",
    "В качестве политики будем использовать нашу обученную DQN:\n",
    "np.argmax(evalQ(s))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalReward = 0\n",
    "s = env.reset()[0]\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    a = np.argmax(evalQ(s)) # выбираем оптимальное действие\n",
    "    s, r, _, done = env.step(a)[:4]\n",
    "    totalReward += r\n",
    "    if done:\n",
    "        env.render()\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "print('Total reward = {}'.format(totalReward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56ea37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
